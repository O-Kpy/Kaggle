{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어 처리 공부.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kVrWorzffGMI"
      ],
      "authorship_tag": "ABX9TyNwKCMesSpma7aF4PnmWEI9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/O-Kpy/Kaggle/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC_%EA%B3%B5%EB%B6%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUs3QyNNltM"
      },
      "source": [
        "# Word embedding\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* Word2vec\n",
        "\n",
        " > 단어가 가지는 의미 자체를 벡터공간으로 가져오는 것\n",
        "\n",
        " > '중심단어'의 '주변단어'들을 이용해 '중심단어'를 추론하는 방식\n",
        " \n",
        " Word2vec의 장점 : 1. 단어간의 관계 추론이 용이, 2. 단어간의 유사성 측정이 용이 3. 벡터 연산을 통한 추론이 가능(ex 한국 - 서울 + 도쿄 = 일본)\n",
        "\n",
        " Word2vec의 단점 : 1.subword information을 무시(서울 vs 서울시 vs 고양시), 2. **Out of Vocabulary(OOV)**에서는 적용 불가능(학습에 사용되지 않은 단어)\n",
        "\n",
        "* FastText(페이스북 오픈소스)\n",
        " \n",
        " > Word2vec의 단점을 커버해줌, Word2vec과 사용법은 거의 비슷\n",
        "\n",
        " > 한국어는 다양한 용언 형태를 가짐 ==> (동사원형:모르다 , 다른 용언 : 모르네, 모르지, 모르구나...)  \n",
        "\n",
        " > 단어의 구성요소를 n_gram으로 나눠 모두 training함 ==> OOV가 나와도 대처 가능\n",
        "\n",
        " > **팁**) 워드 임베딩은 음절 단위(대한민국->대한,한민,민국...,n_gram=2-4), 한국어는 자소단위로 나눠서 함(ㄷ,ㅐ,ㅎ,ㅏ,ㄴ...n_gram=6-12) --> 오탈자에 강하다\n",
        "\n",
        "\n",
        "* Word embedding의 한계점\n",
        "\n",
        " > 다의어, 동형어(같은 글자인데 여러가지 뜻을 가지는 단어) 에 대해선 성능이 안좋음\n",
        "\n",
        " > 주변 단어를 통해 학습되기 때문에 '문맥'을 고려하지 않음\n",
        "\n",
        "```\n",
        "from gensim.models import Word2vec\n",
        "\n",
        "from gensim.models import FastText\n",
        "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "# 학습이 진행되었다면, 이제 electrofishing에 대해서 유사 단어를 찾아보도록 하겠습니다.\n",
        "\n",
        "model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        ">>>[('electrolux', 0.7934642434120178), ('electrolyte', 0.78279709815979), ('electro', 0.779127836227417), ('electric', 0.7753111720085144), ('airbus', 0.7648627758026123), ('fukushima', 0.7612422704696655), ('electrochemical', 0.7611693143844604), ('gastric', 0.7483425140380859), ('electroshock', 0.7477173805236816), ('overfishing', 0.7435552477836609)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVrWorzffGMI"
      },
      "source": [
        "# Word embedding 실습(Word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjw3ovC6WvOO",
        "outputId": "6bfbbf0f-acb5-4427-f683-727a8d42fcee"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 30.9MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVOBKzJpecgp"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from gensim.models import Word2Vec\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSMbFg9GeqdS",
        "outputId": "020f37c4-4ea4-4042-e757-755573c64a6b"
      },
      "source": [
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt', filename='ratings.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings.txt', <http.client.HTTPMessage at 0x7fd644f3ecd0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_td7Oyvne0WA"
      },
      "source": [
        "train = pd.read_table('ratings.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "4wO_8zMDe6RD",
        "outputId": "d8c51462-4bbc-4636-bbcf-01ff0a53c7ed"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
              "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
              "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-1nlPNKfLyT"
      },
      "source": [
        "# 정규표현식으로 한글 외 문자 제거\n",
        "train['document'] = train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")  # ㄱ 에서 ㅎ, ㅏ 에서 ㅣ, 가 에서 힣 외==> 정규 표현식\n",
        "train = train.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4QzFZk8FJGXz",
        "outputId": "a5973d0d-0759-49f4-c4f5-a3460d98928e"
      },
      "source": [
        "train['document'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'어릴때보고 지금다시봐도 재밌어요ㅋㅋ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--CI9LDcfs00"
      },
      "source": [
        "# 불용어 정의(은,는,이,가,...)\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '을']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL7h0W98f7Y0"
      },
      "source": [
        "okt = Okt()\n",
        "tokenized_data = []\n",
        "for sentence in train['document']:\n",
        "    temp_X = okt.morphs(sentence, stem=True)  # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords]  # 불용어 제거\n",
        "    tokenized_data.append(temp_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-STEHrtgZpn",
        "outputId": "58724e86-e07b-45c8-98ea-6cea5a998893"
      },
      "source": [
        "tokenized_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['어리다', '때', '보고', '지금', '다시', '보다', '재밌다', 'ㅋㅋ'],\n",
              " ['디자인',\n",
              "  '을',\n",
              "  '배우다',\n",
              "  '학생',\n",
              "  '외국',\n",
              "  '디자이너',\n",
              "  '그',\n",
              "  '일군',\n",
              "  '전통',\n",
              "  '을',\n",
              "  '통해',\n",
              "  '발전',\n",
              "  '문화',\n",
              "  '산업',\n",
              "  '부럽다',\n",
              "  '사실',\n",
              "  '우리나라',\n",
              "  '에서도',\n",
              "  '그',\n",
              "  '어렵다',\n",
              "  '시절',\n",
              "  '끝',\n",
              "  '까지',\n",
              "  '열정',\n",
              "  '을',\n",
              "  '지키다',\n",
              "  '노라노',\n",
              "  '같다',\n",
              "  '전통',\n",
              "  '있다',\n",
              "  '저',\n",
              "  '같다',\n",
              "  '사람',\n",
              "  '꿈',\n",
              "  '을',\n",
              "  '꾸다',\n",
              "  '이루다',\n",
              "  '나가다',\n",
              "  '수',\n",
              "  '있다',\n",
              "  '것',\n",
              "  '감사하다'],\n",
              " ['폴리스스토리', '시리즈', '부터', '뉴', '까지', '버리다', '하나', '없다', '최고'],\n",
              " ['오다',\n",
              "  '연기',\n",
              "  '진짜',\n",
              "  '개',\n",
              "  '쩔다',\n",
              "  '지루하다',\n",
              "  '생각',\n",
              "  '몰입',\n",
              "  '보다',\n",
              "  '그렇다',\n",
              "  '이렇다',\n",
              "  '진짜',\n",
              "  '영화',\n",
              "  '지'],\n",
              " ['안개', '자욱하다', '밤하늘', '뜨다', '있다', '초승달', '같다', '영화']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8aDCXGSmHCO",
        "outputId": "ae64f47e-4aef-4f21-ed52-8497dde2c8dc"
      },
      "source": [
        "# Word2vec 훈련\n",
        "model = Word2Vec(sentences=tokenized_data, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "model.wv.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16477, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeCGzK3zmiF3",
        "outputId": "88f91ed4-a709-43a3-a45e-face0e15580b"
      },
      "source": [
        "# similarities 보기\n",
        "print(model.wv.most_similar('한석규'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('설경구', 0.8939294219017029), ('엄정화', 0.8905223608016968), ('차승원', 0.8891187906265259), ('김명민', 0.8779950737953186), ('안성기', 0.876231849193573), ('최민수', 0.875919759273529), ('박신양', 0.8747932314872742), ('장미희', 0.8695060014724731), ('김수현', 0.8677871227264404), ('전도연', 0.8588352203369141)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDGcoNVVmovy",
        "outputId": "942057e3-9111-46b6-97c6-295a200c9d18"
      },
      "source": [
        "print(model.wv.most_similar('몰입'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('집중', 0.9063402414321899), ('감정이입', 0.7843761444091797), ('긴장', 0.7079639434814453), ('박진', 0.6695482730865479), ('정선', 0.668513298034668), ('긴박', 0.6657693982124329), ('몰입도', 0.6573964357376099), ('이입', 0.644713282585144), ('이해도', 0.6364505887031555), ('빠져들다', 0.6222038865089417)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxPJFx3am2ce"
      },
      "source": [
        "# 사전 훈련(Pre-trained)된 Word2Vec 임베딩\n",
        "\n",
        "위의 예제같이 직접 Word2Vec 을 만들어 사용할 수 있지만, 시간이 오래걸리고 데이터의 양도 한정적이라는 단점이 있기에 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩을 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용하는 것도 가능하다.\n",
        "\n",
        "이러한 사전 훈련된 Word2Vec 을 Pre-trained Word2Vec Embedding 이라고 한다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* 영어\n",
        "\n",
        "구글에서는 사전 훈련된 3백만 개의 Word2Vec 단어 벡터를 제공한다. 각 임베딩의 차원은 300 이다. gensim을 통해 해당 모델을 불러올 수 있다. 모델을 다운로드 하고 해당 파일 경로에서\n",
        "\n",
        "모델 다운로드 경로 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
        "\n",
        "압축 파일의 용량은 약 1.5GB이지만 압축을 풀면 약 3.3GB의 파일이 나오게된다.\n",
        "\n",
        "\n",
        "```\n",
        "import gensim\n",
        "\n",
        "# 구글의 사전 훈련된 Word2Vec 모델을 로드\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin 파일 경로', binary=True) \n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* 한국어\n",
        "\n",
        "한국어는 박규병님께서 공개한 Word2Vec 모델이 있다. 해당 깃허브 주소와 모델의 다운로드 링크는 아래와 같다.\n",
        "\n",
        "GitHub : https://github.com/Kyubyong/wordvectors\n",
        "모델 다운로드 경로 : https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view\n",
        "\n",
        "파일의 크기는 약 77MB이고 압축을 풀면 50MV 가량의 ko.bin 파일을 gensim 라이브러리로 로드하면 된다.\n",
        "\n",
        "\n",
        "```\n",
        "import gensim\n",
        "model = gensim.models.Word2Vec.load('ko.bin 파일의 경로')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WONN1JiZfipf"
      },
      "source": [
        "# 언어 모델(Language Model, LM)\n",
        " - Markov 확률 기반 언어 모델(마르코프 모델)\n",
        "  - 마코프 체인 모델\n",
        "  - 초기의 언어 모델은 다음의 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산\n",
        "  - 딥러닝 기반의 언어모델은 해당 확률을 최대로 하도록 네트워크를 학습\n",
        "\n",
        " - RNN(Recurrenct Neural Network) 기반 언어 모델\n",
        "  - 이전 state 정보가 다음 state를 예측하는데 사용됨, 시계열 데이터 처리에 특화\n",
        "\n",
        " - seq2seq(Sequence to Sequence)\n",
        "  - 단점 : 1. sequence가 매우 길 경우 처음에 나온 tokens에 대한 정보가 희박(RNN이다 보니), 2. 중요하지 않은 토큰에 힘을 씀 ==> attention모델의 탄생\n",
        "  - Encode layer : RNN구조를 통해 context vector를 획득\n",
        "  - Decode layer : 획득된 context vector를 입력으로 받아서 출력으로 예측\n",
        "\n",
        " - attention모델\n",
        "  - 기존 RNN모델에서 시작\n",
        "  - 출력된 score에 Softmax를 취함 --> 0-1값을 가짐\n",
        "  - 해당 값을 가중치(weight)으로 취함 --> dynamic context vector를 획득가능(다른 언어라도 가중치 때문에)\n",
        "  - RNN모델로 시작하다 보니 순차적으로 연산이 이루어져 느림\n",
        " \n",
        " - self-attention \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WniC1qwJTH23"
      },
      "source": [
        "# Bert(bi-directional Transformer)\n",
        " - 잘 만들어진 BERT 언어모델 위에 1개의 classification layer만 부착하여 다양한 NLP task를 수행\n",
        " - 영어권에서 11개의 NLP에 대해 state-of-the-art 달성\n",
        " - from_pretrained, bert-base-encased, bert-large-encased\n",
        " - [tokenizing] - 빈도수에 기반해 단어를 의미 있는 패턴으로 잘라서 tokenizing\n",
        " - 데이터의 tokenizing\n",
        "  - Masked language model(MLM) : input token을 일정 확률로 masking\n",
        "  - Randomly하게 masking:80%, replace:10, unchange:10%\n",
        "  - Masking 기법 : 숨겨서 학습하게 함\n",
        "  - Bert 적용 : 감성분석(긍정, 부정), 문장 유사도(이진 분류 기반, 문장 벡터 기반)"
      ]
    }
  ]
}