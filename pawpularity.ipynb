{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pawpularity 필사 계속.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNX/Ra+hsEWubP5nPPAEUIx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/O-Kpy/Kaggle/blob/main/pawpularity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Op9Ou26g9s"
      },
      "source": [
        "!pip install timm\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoxXlPju79BR"
      },
      "source": [
        "!pip install albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jr-vFon6xrm"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda import amp\n",
        "\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "import torchvision.transforms as T\n",
        "from albumentations.pytorch import ToTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXNGwReh8Gz9"
      },
      "source": [
        "import wandb\n",
        "\n",
        "try:\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  api_key = user_secrets.get_secret('wandb_api')\n",
        "  wandb.login(key=api_key)\n",
        "  anony = None\n",
        "except:\n",
        "  anony = 'must'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zphvVW5U9AWg"
      },
      "source": [
        "ROOT_DIR = 'Pawpularity'\n",
        "TRAIN_DIR = 'Pawpularity/train'\n",
        "TEST_DIR = 'Pawpularity/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TfO0WH69NP6"
      },
      "source": [
        "CONFIG = dict(\n",
        "    seed = 42,\n",
        "    model_name = 'tf_efficientnet_b4_ns',\n",
        "    train_batch_size = 16,\n",
        "    valid_batch_size = 32,\n",
        "    img_size = 512,\n",
        "    epochs = 10,\n",
        "    learning_rate = 1e-5,\n",
        "    scheduler = 'CosineAnnealingWarmRestarts',\n",
        "    min_lr = 1e-4,\n",
        "    T_max = 20,\n",
        "    T_0 = 20,\n",
        "    warmup_epochs = 0,\n",
        "    weight_decay = 1e-6,\n",
        "    n_accumulate = 1,\n",
        "    n_fold = 5,\n",
        "    num_classes = 1,\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    competition = 'PetFinder',\n",
        "    wandb_kernel = 'deb'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Bb7uSV-l3D"
      },
      "source": [
        "def set_seed(seed = 42):\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(CONFIG['seed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDaqGCju-8-1"
      },
      "source": [
        "def get_train_file_path(id):\n",
        "  return f'{TRAIN_DIR}/{id}.jpg'\n",
        "\n",
        "df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n",
        "df['file_path'] = df['Id'].apply(get_train_file_path)\n",
        "\n",
        "feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAzt1t4H_pnk"
      },
      "source": [
        "def create_folds(df, n_s, n_grp=None):\n",
        "  df['kfold'] = -1\n",
        "\n",
        "  if n_grp is None:\n",
        "    skf = KFold(n_splits=n_s, random_state=CONFIG['seed'])\n",
        "    target = df['Pawpularity']\n",
        "\n",
        "  else:\n",
        "    skf = StratifiedKFold(n_splits=n_s, random_state=CONFIG['seed'], shuffle=True)\n",
        "    df['grp'] = pd.cut(df['Pawpularity'], bins=n_grp, labels=False)\n",
        "    target=df['grp']\n",
        "\n",
        "  for fold_no, (t, v) in enumerate(skf.split(target, target)):\n",
        "    df.loc[v, 'kfold'] = fold_no\n",
        "\n",
        "  df = df.drop('grp', axis=1)\n",
        "\n",
        "  return df\n",
        "\n",
        "df = create_folds(df, n_s=CONFIG+['n_fold'], n_grp=14)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fd-XzPiG1XY"
      },
      "source": [
        "class PawpularityDataset(Dataset):\n",
        "  def __init__(self, root_dir, df, transforms=None):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = df\n",
        "    self.file_name = df['file_path'].values\n",
        "    self.targets = df['Pawpularity'].values\n",
        "    self.meta = df[feature_cols].values\n",
        "    self.transforms = transfomrs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = self.file_name[index]\n",
        "    img = cv2.imread(img_path)  # 이미지 읽고\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB color로 변환\n",
        "    meta = self.meta[index, :]\n",
        "    target = self.targets[index]\n",
        "\n",
        "    if self.transforms:\n",
        "      img = self.transforms(image=img)['image']\n",
        "\n",
        "    return img, meta, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAsgk5V_PGLC"
      },
      "source": [
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "data_transforms = {\n",
        "    'train' : T.Compose([\n",
        "                         T.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
        "                         T.RandomHorizontalFlip(),\n",
        "                         T.RandomVerticalFlip(),\n",
        "                         T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "                         T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                         T.Normalize(mean=MEAN, std=STD)\n",
        "    ]),\n",
        "    'valid' : T.Compose([\n",
        "                         T.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
        "                         T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_transform = {\n",
        "    'train' : A.Compose([\n",
        "                         A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
        "                         A.HorizontalFlip(p=0.5),\n",
        "                         A.Normalize(mean=MEAN, std=STD, p=1.0, max_pixel_value=255.0),\n",
        "                         ToTensor()\n",
        "    ], p=1.0),\n",
        "    'valid' : A.Compose([\n",
        "                         A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
        "                         A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0, p=1.0),\n",
        "                         ToTensor()\n",
        "    ], p=1.0)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RTcY84mSiSN"
      },
      "source": [
        "class PawpularityModel(nn.Module):\n",
        "  def __init__(self, model_name, pretrained=True):\n",
        "    super(PawpularityModel, self).__init__()\n",
        "    self.model = timm.create_model(model_name, pretrained=pretrained)\n",
        "    self.n_features = self.model.classifier.in_features\n",
        "    self.model.reset_classifier(0)\n",
        "    self.fc = nn.Linear(self.n_features + 12, CONFIG['num_classes'])\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "  def forward(self, images, meta):\n",
        "    features = self.model(images)\n",
        "    features = self.dropout(features)\n",
        "    features = torch.cat([features, meta], dim=1)\n",
        "    output = self.fc(features)\n",
        "    return output\n",
        "\n",
        "model = PawpularityModel(CONFIG['model_name'])\n",
        "model.to(CONFIG['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB4IhxRDUfGr"
      },
      "source": [
        "def criterion(outputs, targets):\n",
        "  return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVOgAmMcMP8S"
      },
      "source": [
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "  model.train()\n",
        "  scaler = amp.GradScaler()\n",
        "\n",
        "  dataset_size=0\n",
        "  running_loss=0.0\n",
        "\n",
        "  bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "  for step, (images, meta, targets) in bar:\n",
        "    images = images.to(device, dtype=torch.float)\n",
        "    meta = meta.to(device, dtype=torch.float)\n",
        "    targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "    batch_size = images.size(0)\n",
        "\n",
        "    with amp.autocast(enabled=True):\n",
        "      outputs = model(images, meta)\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss = loss/CONFIG['n_accumulate']\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "    running_loss += (loss.item() * batch_size)\n",
        "    dataset_size += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "\n",
        "    bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n",
        "  gc.collect()\n",
        "\n",
        "  return epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdRV34bBXxbt"
      },
      "source": [
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "  model.eval()\n",
        "\n",
        "  dataset_size=0\n",
        "  running_loss=0.0\n",
        "\n",
        "  TARGETS=[]\n",
        "  PREDS=[]\n",
        "\n",
        "  bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "  for step, (images, meta, targets) in bar:\n",
        "    images = images.to(device, dtype=torch.float)\n",
        "    meta = meta.to(device, dtype=torch.float)\n",
        "    targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "    batch_size = images.size(0)\n",
        "\n",
        "    outputs = model(images, meta)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    running_loss += (loss.item() * batch_size)\n",
        "    dataset_size += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "\n",
        "    PREDS.append(outputs.view(-1).cpu().detach().numpy())\n",
        "    TARGETS.append(targets.view(-1).cpu().detach().numpy())\n",
        "\n",
        "    bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n",
        "\n",
        "  TARGETS = np.concatenate(TARGETS)\n",
        "  PREDS = np.concatenate(PREDS)\n",
        "  val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n",
        "  gc.collect()\n",
        "\n",
        "  return epoch_loss, val_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdkCoRuxZHHZ"
      },
      "source": [
        "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
        "  wandb.watch(model, log_freq=100)\n",
        "\n",
        "  start = time.time()\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_epoch_rmse = np.inf\n",
        "  history = defaultdict(list)\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    gc.collect()\n",
        "    train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader=dataloader,\n",
        "                                       device=CONFIG['device'], epoch=epochs)\n",
        "    \n",
        "    val_epoch_loss, val_epoch_rmse = valid_one_epoch(model, valid_loader, device=CONFIG['device'], epoch=epoch)\n",
        "\n",
        "\n",
        "    history['Train Loss'].append(train_epoch_loss)\n",
        "    history['Valid Loss'].append(val_epoch_loss)\n",
        "    history['Valid RMSE'].append(val_epoch_rmse)\n",
        "\n",
        "    wandb.log({'Train Loss' : train_epoch_loss})\n",
        "    wandb.log({'Valid Loss' : val_epoch_loss})\n",
        "    wandb.log({'Valid RMSE' : val_epoch_rmse})\n",
        "\n",
        "    print(f'Valid RMSE : {val_epoch_rmse}')\n",
        "\n",
        "    if val_epoch_rmse <= best_epoch_rmse:\n",
        "      print(f'{c_}Validation Loss Improved ({best_epoch_rmse} --> {val_epoch_rmse})')\n",
        "      best_epoch_rmse = val_epoch_rmse\n",
        "      run.summary['Best RMSE'] = best_epoch_rmse\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      PATH = 'epoch{:.0f}.bin'.format(epoch)\n",
        "      torch.save(model.state_dict(), PATH)\n",
        "      wandb.save(PATH)\n",
        "      print(f'MODEL Saved{sr_}')\n",
        "\n",
        "    print()\n",
        "\n",
        "  end = time.time()\n",
        "  time_elapsed = end - start\n",
        "  print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "      time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "  print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACrd8LHRdfVI"
      },
      "source": [
        "def prepare_loader(fold):\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n",
        "  valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=4, shuffle=False, pin_memory=True)\n",
        "\n",
        "  return train_loader, valid_loader\n",
        "\n",
        "train_loader, valid_loader = prepare_loader(fold=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ne0xqK3ebNd"
      },
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], eta_min=CONFIG['min_lr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfKI-711e0-v"
      },
      "source": [
        "# wandb 모델링 시작할거란\n",
        "run = wandb.init(project='Pawpularity',\n",
        "                 config=CONFIG,\n",
        "                 job_type='Train',\n",
        "                 anonymous='must')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEadTB1ve805"
      },
      "source": [
        "model, history = run_training(model, optimizer, scheduler, device=CONFIG['device'], num_epochs=CONFIG['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZphirMYfJ9Y"
      },
      "source": [
        "# wandb run finish\n",
        "run.finist"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}