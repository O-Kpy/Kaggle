{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "santander_prediction.ipynb",
      "provenance": [],
      "mount_file_id": "1awGektg8tI_O9n4FhrGFy41XS1qo2qpS",
      "authorship_tag": "ABX9TyOKwv29Y/OODlpAVVelznvm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/O-Kpy/Kaggle/blob/main/santander_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0nIu0BwiW53"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy.stats import skew"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIxISfUtikbr"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/dataset/kaggle/santander_train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/dataset/kaggle/santander_test.csv')\n",
        "submit = pd.read_csv('/content/drive/MyDrive/dataset/kaggle/santander_sample_submission.csv')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC2KjiTvjZZ9"
      },
      "source": [
        "data = pd.concat([train, test], axis=0)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgzfkoLzkckz",
        "outputId": "90c9be30-13fd-4f5e-e091-883a5e6abb5b"
      },
      "source": [
        "data.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target     200000\n",
              "var_199         0\n",
              "var_61          0\n",
              "var_71          0\n",
              "var_70          0\n",
              "            ...  \n",
              "var_129         0\n",
              "var_128         0\n",
              "var_127         0\n",
              "var_126         0\n",
              "ID_code         0\n",
              "Length: 202, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bwp1uL5klHt",
        "outputId": "d6a88097-2b33-4103-d4b6-4027ce7c7256"
      },
      "source": [
        "data.skew().sort_values(ascending=False)\n",
        "# 너무 feature가 잘 다듬어져 있는데"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target     2.657642\n",
              "var_168    0.268065\n",
              "var_2      0.261597\n",
              "var_179    0.245241\n",
              "var_163    0.238993\n",
              "             ...   \n",
              "var_80    -0.215633\n",
              "var_86    -0.217701\n",
              "var_81    -0.233976\n",
              "var_93    -0.241298\n",
              "var_44    -0.334506\n",
              "Length: 201, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ej0O2grle6S"
      },
      "source": [
        "# 여기서 더 할 feature engineering은 polynominal 변환, aggregation밖에 없지 않을까?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ZsVVLzNSkwB7",
        "outputId": "d8847bb8-e5de-4c5d-95df-ef1b052914f5"
      },
      "source": [
        "train.target.hist()\n",
        "# 데이터 불균형이 존재하긴 하네\n",
        "# 취할 수 있는 액션 ==> 언더샘플링, 오버샘플링(이건 오버피팅 나니까 하지말자)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8a880a0650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "qIH7b3cYmogK",
        "outputId": "32736105-1059-4e1b-a6ee-76cd55dbbd1f"
      },
      "source": [
        "train"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_code</th>\n",
              "      <th>target</th>\n",
              "      <th>var_0</th>\n",
              "      <th>var_1</th>\n",
              "      <th>var_2</th>\n",
              "      <th>var_3</th>\n",
              "      <th>var_4</th>\n",
              "      <th>var_5</th>\n",
              "      <th>var_6</th>\n",
              "      <th>var_7</th>\n",
              "      <th>var_8</th>\n",
              "      <th>var_9</th>\n",
              "      <th>var_10</th>\n",
              "      <th>var_11</th>\n",
              "      <th>var_12</th>\n",
              "      <th>var_13</th>\n",
              "      <th>var_14</th>\n",
              "      <th>var_15</th>\n",
              "      <th>var_16</th>\n",
              "      <th>var_17</th>\n",
              "      <th>var_18</th>\n",
              "      <th>var_19</th>\n",
              "      <th>var_20</th>\n",
              "      <th>var_21</th>\n",
              "      <th>var_22</th>\n",
              "      <th>var_23</th>\n",
              "      <th>var_24</th>\n",
              "      <th>var_25</th>\n",
              "      <th>var_26</th>\n",
              "      <th>var_27</th>\n",
              "      <th>var_28</th>\n",
              "      <th>var_29</th>\n",
              "      <th>var_30</th>\n",
              "      <th>var_31</th>\n",
              "      <th>var_32</th>\n",
              "      <th>var_33</th>\n",
              "      <th>var_34</th>\n",
              "      <th>var_35</th>\n",
              "      <th>var_36</th>\n",
              "      <th>var_37</th>\n",
              "      <th>...</th>\n",
              "      <th>var_160</th>\n",
              "      <th>var_161</th>\n",
              "      <th>var_162</th>\n",
              "      <th>var_163</th>\n",
              "      <th>var_164</th>\n",
              "      <th>var_165</th>\n",
              "      <th>var_166</th>\n",
              "      <th>var_167</th>\n",
              "      <th>var_168</th>\n",
              "      <th>var_169</th>\n",
              "      <th>var_170</th>\n",
              "      <th>var_171</th>\n",
              "      <th>var_172</th>\n",
              "      <th>var_173</th>\n",
              "      <th>var_174</th>\n",
              "      <th>var_175</th>\n",
              "      <th>var_176</th>\n",
              "      <th>var_177</th>\n",
              "      <th>var_178</th>\n",
              "      <th>var_179</th>\n",
              "      <th>var_180</th>\n",
              "      <th>var_181</th>\n",
              "      <th>var_182</th>\n",
              "      <th>var_183</th>\n",
              "      <th>var_184</th>\n",
              "      <th>var_185</th>\n",
              "      <th>var_186</th>\n",
              "      <th>var_187</th>\n",
              "      <th>var_188</th>\n",
              "      <th>var_189</th>\n",
              "      <th>var_190</th>\n",
              "      <th>var_191</th>\n",
              "      <th>var_192</th>\n",
              "      <th>var_193</th>\n",
              "      <th>var_194</th>\n",
              "      <th>var_195</th>\n",
              "      <th>var_196</th>\n",
              "      <th>var_197</th>\n",
              "      <th>var_198</th>\n",
              "      <th>var_199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.9255</td>\n",
              "      <td>-6.7863</td>\n",
              "      <td>11.9081</td>\n",
              "      <td>5.0930</td>\n",
              "      <td>11.4607</td>\n",
              "      <td>-9.2834</td>\n",
              "      <td>5.1187</td>\n",
              "      <td>18.6266</td>\n",
              "      <td>-4.9200</td>\n",
              "      <td>5.7470</td>\n",
              "      <td>2.9252</td>\n",
              "      <td>3.1821</td>\n",
              "      <td>14.0137</td>\n",
              "      <td>0.5745</td>\n",
              "      <td>8.7989</td>\n",
              "      <td>14.5691</td>\n",
              "      <td>5.7487</td>\n",
              "      <td>-7.2393</td>\n",
              "      <td>4.2840</td>\n",
              "      <td>30.7133</td>\n",
              "      <td>10.5350</td>\n",
              "      <td>16.2191</td>\n",
              "      <td>2.5791</td>\n",
              "      <td>2.4716</td>\n",
              "      <td>14.3831</td>\n",
              "      <td>13.4325</td>\n",
              "      <td>-5.1488</td>\n",
              "      <td>-0.4073</td>\n",
              "      <td>4.9306</td>\n",
              "      <td>5.9965</td>\n",
              "      <td>-0.3085</td>\n",
              "      <td>12.9041</td>\n",
              "      <td>-3.8766</td>\n",
              "      <td>16.8911</td>\n",
              "      <td>11.1920</td>\n",
              "      <td>10.5785</td>\n",
              "      <td>0.6764</td>\n",
              "      <td>7.8871</td>\n",
              "      <td>...</td>\n",
              "      <td>15.4576</td>\n",
              "      <td>5.3133</td>\n",
              "      <td>3.6159</td>\n",
              "      <td>5.0384</td>\n",
              "      <td>6.6760</td>\n",
              "      <td>12.6644</td>\n",
              "      <td>2.7004</td>\n",
              "      <td>-0.6975</td>\n",
              "      <td>9.5981</td>\n",
              "      <td>5.4879</td>\n",
              "      <td>-4.7645</td>\n",
              "      <td>-8.4254</td>\n",
              "      <td>20.8773</td>\n",
              "      <td>3.1531</td>\n",
              "      <td>18.5618</td>\n",
              "      <td>7.7423</td>\n",
              "      <td>-10.1245</td>\n",
              "      <td>13.7241</td>\n",
              "      <td>-3.5189</td>\n",
              "      <td>1.7202</td>\n",
              "      <td>-8.4051</td>\n",
              "      <td>9.0164</td>\n",
              "      <td>3.0657</td>\n",
              "      <td>14.3691</td>\n",
              "      <td>25.8398</td>\n",
              "      <td>5.8764</td>\n",
              "      <td>11.8411</td>\n",
              "      <td>-19.7159</td>\n",
              "      <td>17.5743</td>\n",
              "      <td>0.5857</td>\n",
              "      <td>4.4354</td>\n",
              "      <td>3.9642</td>\n",
              "      <td>3.1364</td>\n",
              "      <td>1.6910</td>\n",
              "      <td>18.5227</td>\n",
              "      <td>-2.3978</td>\n",
              "      <td>7.8784</td>\n",
              "      <td>8.5635</td>\n",
              "      <td>12.7803</td>\n",
              "      <td>-1.0914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>0</td>\n",
              "      <td>11.5006</td>\n",
              "      <td>-4.1473</td>\n",
              "      <td>13.8588</td>\n",
              "      <td>5.3890</td>\n",
              "      <td>12.3622</td>\n",
              "      <td>7.0433</td>\n",
              "      <td>5.6208</td>\n",
              "      <td>16.5338</td>\n",
              "      <td>3.1468</td>\n",
              "      <td>8.0851</td>\n",
              "      <td>-0.4032</td>\n",
              "      <td>8.0585</td>\n",
              "      <td>14.0239</td>\n",
              "      <td>8.4135</td>\n",
              "      <td>5.4345</td>\n",
              "      <td>13.7003</td>\n",
              "      <td>13.8275</td>\n",
              "      <td>-15.5849</td>\n",
              "      <td>7.8000</td>\n",
              "      <td>28.5708</td>\n",
              "      <td>3.4287</td>\n",
              "      <td>2.7407</td>\n",
              "      <td>8.5524</td>\n",
              "      <td>3.3716</td>\n",
              "      <td>6.9779</td>\n",
              "      <td>13.8910</td>\n",
              "      <td>-11.7684</td>\n",
              "      <td>-2.5586</td>\n",
              "      <td>5.0464</td>\n",
              "      <td>0.5481</td>\n",
              "      <td>-9.2987</td>\n",
              "      <td>7.8755</td>\n",
              "      <td>1.2859</td>\n",
              "      <td>19.3710</td>\n",
              "      <td>11.3702</td>\n",
              "      <td>0.7399</td>\n",
              "      <td>2.7995</td>\n",
              "      <td>5.8434</td>\n",
              "      <td>...</td>\n",
              "      <td>29.4846</td>\n",
              "      <td>5.8683</td>\n",
              "      <td>3.8208</td>\n",
              "      <td>15.8348</td>\n",
              "      <td>-5.0121</td>\n",
              "      <td>15.1345</td>\n",
              "      <td>3.2003</td>\n",
              "      <td>9.3192</td>\n",
              "      <td>3.8821</td>\n",
              "      <td>5.7999</td>\n",
              "      <td>5.5378</td>\n",
              "      <td>5.0988</td>\n",
              "      <td>22.0330</td>\n",
              "      <td>5.5134</td>\n",
              "      <td>30.2645</td>\n",
              "      <td>10.4968</td>\n",
              "      <td>-7.2352</td>\n",
              "      <td>16.5721</td>\n",
              "      <td>-7.3477</td>\n",
              "      <td>11.0752</td>\n",
              "      <td>-5.5937</td>\n",
              "      <td>9.4878</td>\n",
              "      <td>-14.9100</td>\n",
              "      <td>9.4245</td>\n",
              "      <td>22.5441</td>\n",
              "      <td>-4.8622</td>\n",
              "      <td>7.6543</td>\n",
              "      <td>-15.9319</td>\n",
              "      <td>13.3175</td>\n",
              "      <td>-0.3566</td>\n",
              "      <td>7.6421</td>\n",
              "      <td>7.7214</td>\n",
              "      <td>2.5837</td>\n",
              "      <td>10.9516</td>\n",
              "      <td>15.4305</td>\n",
              "      <td>2.0339</td>\n",
              "      <td>8.1267</td>\n",
              "      <td>8.7889</td>\n",
              "      <td>18.3560</td>\n",
              "      <td>1.9518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>0</td>\n",
              "      <td>8.6093</td>\n",
              "      <td>-2.7457</td>\n",
              "      <td>12.0805</td>\n",
              "      <td>7.8928</td>\n",
              "      <td>10.5825</td>\n",
              "      <td>-9.0837</td>\n",
              "      <td>6.9427</td>\n",
              "      <td>14.6155</td>\n",
              "      <td>-4.9193</td>\n",
              "      <td>5.9525</td>\n",
              "      <td>-0.3249</td>\n",
              "      <td>-11.2648</td>\n",
              "      <td>14.1929</td>\n",
              "      <td>7.3124</td>\n",
              "      <td>7.5244</td>\n",
              "      <td>14.6472</td>\n",
              "      <td>7.6782</td>\n",
              "      <td>-1.7395</td>\n",
              "      <td>4.7011</td>\n",
              "      <td>20.4775</td>\n",
              "      <td>17.7559</td>\n",
              "      <td>18.1377</td>\n",
              "      <td>1.2145</td>\n",
              "      <td>3.5137</td>\n",
              "      <td>5.6777</td>\n",
              "      <td>13.2177</td>\n",
              "      <td>-7.9940</td>\n",
              "      <td>-2.9029</td>\n",
              "      <td>5.8463</td>\n",
              "      <td>6.1439</td>\n",
              "      <td>-11.1025</td>\n",
              "      <td>12.4858</td>\n",
              "      <td>-2.2871</td>\n",
              "      <td>19.0422</td>\n",
              "      <td>11.0449</td>\n",
              "      <td>4.1087</td>\n",
              "      <td>4.6974</td>\n",
              "      <td>6.9346</td>\n",
              "      <td>...</td>\n",
              "      <td>13.2070</td>\n",
              "      <td>5.8442</td>\n",
              "      <td>4.7086</td>\n",
              "      <td>5.7141</td>\n",
              "      <td>-1.0410</td>\n",
              "      <td>20.5092</td>\n",
              "      <td>3.2790</td>\n",
              "      <td>-5.5952</td>\n",
              "      <td>7.3176</td>\n",
              "      <td>5.7690</td>\n",
              "      <td>-7.0927</td>\n",
              "      <td>-3.9116</td>\n",
              "      <td>7.2569</td>\n",
              "      <td>-5.8234</td>\n",
              "      <td>25.6820</td>\n",
              "      <td>10.9202</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>8.8438</td>\n",
              "      <td>-9.7009</td>\n",
              "      <td>2.4013</td>\n",
              "      <td>-4.2935</td>\n",
              "      <td>9.3908</td>\n",
              "      <td>-13.2648</td>\n",
              "      <td>3.1545</td>\n",
              "      <td>23.0866</td>\n",
              "      <td>-5.3000</td>\n",
              "      <td>5.3745</td>\n",
              "      <td>-6.2660</td>\n",
              "      <td>10.1934</td>\n",
              "      <td>-0.8417</td>\n",
              "      <td>2.9057</td>\n",
              "      <td>9.7905</td>\n",
              "      <td>1.6704</td>\n",
              "      <td>1.6858</td>\n",
              "      <td>21.6042</td>\n",
              "      <td>3.1417</td>\n",
              "      <td>-6.5213</td>\n",
              "      <td>8.2675</td>\n",
              "      <td>14.7222</td>\n",
              "      <td>0.3965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0604</td>\n",
              "      <td>-2.1518</td>\n",
              "      <td>8.9522</td>\n",
              "      <td>7.1957</td>\n",
              "      <td>12.5846</td>\n",
              "      <td>-1.8361</td>\n",
              "      <td>5.8428</td>\n",
              "      <td>14.9250</td>\n",
              "      <td>-5.8609</td>\n",
              "      <td>8.2450</td>\n",
              "      <td>2.3061</td>\n",
              "      <td>2.8102</td>\n",
              "      <td>13.8463</td>\n",
              "      <td>11.9704</td>\n",
              "      <td>6.4569</td>\n",
              "      <td>14.8372</td>\n",
              "      <td>10.7430</td>\n",
              "      <td>-0.4299</td>\n",
              "      <td>15.9426</td>\n",
              "      <td>13.7257</td>\n",
              "      <td>20.3010</td>\n",
              "      <td>12.5579</td>\n",
              "      <td>6.8202</td>\n",
              "      <td>2.7229</td>\n",
              "      <td>12.1354</td>\n",
              "      <td>13.7367</td>\n",
              "      <td>0.8135</td>\n",
              "      <td>-0.9059</td>\n",
              "      <td>5.9070</td>\n",
              "      <td>2.8407</td>\n",
              "      <td>-15.2398</td>\n",
              "      <td>10.4407</td>\n",
              "      <td>-2.5731</td>\n",
              "      <td>6.1796</td>\n",
              "      <td>10.6093</td>\n",
              "      <td>-5.9158</td>\n",
              "      <td>8.1723</td>\n",
              "      <td>2.8521</td>\n",
              "      <td>...</td>\n",
              "      <td>31.8833</td>\n",
              "      <td>5.9684</td>\n",
              "      <td>7.2084</td>\n",
              "      <td>3.8899</td>\n",
              "      <td>-11.0882</td>\n",
              "      <td>17.2502</td>\n",
              "      <td>2.5881</td>\n",
              "      <td>-2.7018</td>\n",
              "      <td>0.5641</td>\n",
              "      <td>5.3430</td>\n",
              "      <td>-7.1541</td>\n",
              "      <td>-6.1920</td>\n",
              "      <td>18.2366</td>\n",
              "      <td>11.7134</td>\n",
              "      <td>14.7483</td>\n",
              "      <td>8.1013</td>\n",
              "      <td>11.8771</td>\n",
              "      <td>13.9552</td>\n",
              "      <td>-10.4701</td>\n",
              "      <td>5.6961</td>\n",
              "      <td>-3.7546</td>\n",
              "      <td>8.4117</td>\n",
              "      <td>1.8986</td>\n",
              "      <td>7.2601</td>\n",
              "      <td>-0.4639</td>\n",
              "      <td>-0.0498</td>\n",
              "      <td>7.9336</td>\n",
              "      <td>-12.8279</td>\n",
              "      <td>12.4124</td>\n",
              "      <td>1.8489</td>\n",
              "      <td>4.4666</td>\n",
              "      <td>4.7433</td>\n",
              "      <td>0.7178</td>\n",
              "      <td>1.4214</td>\n",
              "      <td>23.0347</td>\n",
              "      <td>-1.2706</td>\n",
              "      <td>-2.9275</td>\n",
              "      <td>10.2922</td>\n",
              "      <td>17.9697</td>\n",
              "      <td>-8.9996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>0</td>\n",
              "      <td>9.8369</td>\n",
              "      <td>-1.4834</td>\n",
              "      <td>12.8746</td>\n",
              "      <td>6.6375</td>\n",
              "      <td>12.2772</td>\n",
              "      <td>2.4486</td>\n",
              "      <td>5.9405</td>\n",
              "      <td>19.2514</td>\n",
              "      <td>6.2654</td>\n",
              "      <td>7.6784</td>\n",
              "      <td>-9.4458</td>\n",
              "      <td>-12.1419</td>\n",
              "      <td>13.8481</td>\n",
              "      <td>7.8895</td>\n",
              "      <td>7.7894</td>\n",
              "      <td>15.0553</td>\n",
              "      <td>8.4871</td>\n",
              "      <td>-3.0680</td>\n",
              "      <td>6.5263</td>\n",
              "      <td>11.3152</td>\n",
              "      <td>21.4246</td>\n",
              "      <td>18.9608</td>\n",
              "      <td>10.1102</td>\n",
              "      <td>2.7142</td>\n",
              "      <td>14.2080</td>\n",
              "      <td>13.5433</td>\n",
              "      <td>3.1736</td>\n",
              "      <td>-3.3423</td>\n",
              "      <td>5.9015</td>\n",
              "      <td>7.9352</td>\n",
              "      <td>-3.1582</td>\n",
              "      <td>9.4668</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>19.3239</td>\n",
              "      <td>12.4057</td>\n",
              "      <td>0.6329</td>\n",
              "      <td>2.7922</td>\n",
              "      <td>5.8184</td>\n",
              "      <td>...</td>\n",
              "      <td>33.5107</td>\n",
              "      <td>5.6953</td>\n",
              "      <td>5.4663</td>\n",
              "      <td>18.2201</td>\n",
              "      <td>6.5769</td>\n",
              "      <td>21.2607</td>\n",
              "      <td>3.2304</td>\n",
              "      <td>-1.7759</td>\n",
              "      <td>3.1283</td>\n",
              "      <td>5.5518</td>\n",
              "      <td>1.4493</td>\n",
              "      <td>-2.6627</td>\n",
              "      <td>19.8056</td>\n",
              "      <td>2.3705</td>\n",
              "      <td>18.4685</td>\n",
              "      <td>16.3309</td>\n",
              "      <td>-3.3456</td>\n",
              "      <td>13.5261</td>\n",
              "      <td>1.7189</td>\n",
              "      <td>5.1743</td>\n",
              "      <td>-7.6938</td>\n",
              "      <td>9.7685</td>\n",
              "      <td>4.8910</td>\n",
              "      <td>12.2198</td>\n",
              "      <td>11.8503</td>\n",
              "      <td>-7.8931</td>\n",
              "      <td>6.4209</td>\n",
              "      <td>5.9270</td>\n",
              "      <td>16.0201</td>\n",
              "      <td>-0.2829</td>\n",
              "      <td>-1.4905</td>\n",
              "      <td>9.5214</td>\n",
              "      <td>-0.1508</td>\n",
              "      <td>9.1942</td>\n",
              "      <td>13.2876</td>\n",
              "      <td>-1.5121</td>\n",
              "      <td>3.9267</td>\n",
              "      <td>9.5031</td>\n",
              "      <td>17.9974</td>\n",
              "      <td>-8.8104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199995</th>\n",
              "      <td>train_199995</td>\n",
              "      <td>0</td>\n",
              "      <td>11.4880</td>\n",
              "      <td>-0.4956</td>\n",
              "      <td>8.2622</td>\n",
              "      <td>3.5142</td>\n",
              "      <td>10.3404</td>\n",
              "      <td>11.6081</td>\n",
              "      <td>5.6709</td>\n",
              "      <td>15.1516</td>\n",
              "      <td>-0.6209</td>\n",
              "      <td>5.6669</td>\n",
              "      <td>3.7574</td>\n",
              "      <td>-9.5348</td>\n",
              "      <td>13.9860</td>\n",
              "      <td>5.2982</td>\n",
              "      <td>8.2705</td>\n",
              "      <td>14.1527</td>\n",
              "      <td>7.4540</td>\n",
              "      <td>-5.0105</td>\n",
              "      <td>12.0465</td>\n",
              "      <td>8.6349</td>\n",
              "      <td>9.9137</td>\n",
              "      <td>25.1376</td>\n",
              "      <td>1.0914</td>\n",
              "      <td>3.2326</td>\n",
              "      <td>7.7802</td>\n",
              "      <td>13.9939</td>\n",
              "      <td>2.9085</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>4.2369</td>\n",
              "      <td>7.5665</td>\n",
              "      <td>-9.2149</td>\n",
              "      <td>9.5746</td>\n",
              "      <td>1.4012</td>\n",
              "      <td>7.4211</td>\n",
              "      <td>11.0075</td>\n",
              "      <td>7.8080</td>\n",
              "      <td>4.5567</td>\n",
              "      <td>4.9861</td>\n",
              "      <td>...</td>\n",
              "      <td>35.4923</td>\n",
              "      <td>5.5477</td>\n",
              "      <td>7.4244</td>\n",
              "      <td>12.5459</td>\n",
              "      <td>-6.7840</td>\n",
              "      <td>31.1895</td>\n",
              "      <td>2.6529</td>\n",
              "      <td>-11.1867</td>\n",
              "      <td>9.8865</td>\n",
              "      <td>5.4730</td>\n",
              "      <td>-5.3880</td>\n",
              "      <td>-0.4698</td>\n",
              "      <td>24.4025</td>\n",
              "      <td>-5.4493</td>\n",
              "      <td>11.3529</td>\n",
              "      <td>7.7075</td>\n",
              "      <td>-5.0491</td>\n",
              "      <td>13.0756</td>\n",
              "      <td>15.8271</td>\n",
              "      <td>3.3580</td>\n",
              "      <td>-14.3371</td>\n",
              "      <td>10.4421</td>\n",
              "      <td>7.6530</td>\n",
              "      <td>9.4585</td>\n",
              "      <td>22.7783</td>\n",
              "      <td>-4.0305</td>\n",
              "      <td>4.2233</td>\n",
              "      <td>-6.3906</td>\n",
              "      <td>13.5058</td>\n",
              "      <td>-0.4594</td>\n",
              "      <td>6.1415</td>\n",
              "      <td>13.2305</td>\n",
              "      <td>3.9901</td>\n",
              "      <td>0.9388</td>\n",
              "      <td>18.0249</td>\n",
              "      <td>-1.7939</td>\n",
              "      <td>2.1661</td>\n",
              "      <td>8.5326</td>\n",
              "      <td>16.6660</td>\n",
              "      <td>-17.8661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199996</th>\n",
              "      <td>train_199996</td>\n",
              "      <td>0</td>\n",
              "      <td>4.9149</td>\n",
              "      <td>-2.4484</td>\n",
              "      <td>16.7052</td>\n",
              "      <td>6.6345</td>\n",
              "      <td>8.3096</td>\n",
              "      <td>-10.5628</td>\n",
              "      <td>5.8802</td>\n",
              "      <td>21.5940</td>\n",
              "      <td>-3.6797</td>\n",
              "      <td>6.0019</td>\n",
              "      <td>6.5576</td>\n",
              "      <td>-11.8776</td>\n",
              "      <td>14.4131</td>\n",
              "      <td>3.3087</td>\n",
              "      <td>3.5800</td>\n",
              "      <td>14.1597</td>\n",
              "      <td>7.5191</td>\n",
              "      <td>-8.8715</td>\n",
              "      <td>17.9467</td>\n",
              "      <td>17.0237</td>\n",
              "      <td>6.6459</td>\n",
              "      <td>18.2345</td>\n",
              "      <td>0.8982</td>\n",
              "      <td>2.2532</td>\n",
              "      <td>15.4977</td>\n",
              "      <td>13.3282</td>\n",
              "      <td>5.2281</td>\n",
              "      <td>-3.7424</td>\n",
              "      <td>5.5144</td>\n",
              "      <td>5.7148</td>\n",
              "      <td>-13.7470</td>\n",
              "      <td>7.4369</td>\n",
              "      <td>1.3041</td>\n",
              "      <td>12.7552</td>\n",
              "      <td>12.5362</td>\n",
              "      <td>-1.1002</td>\n",
              "      <td>2.4370</td>\n",
              "      <td>6.2631</td>\n",
              "      <td>...</td>\n",
              "      <td>35.1445</td>\n",
              "      <td>5.5375</td>\n",
              "      <td>5.6397</td>\n",
              "      <td>17.0598</td>\n",
              "      <td>-9.7142</td>\n",
              "      <td>15.5117</td>\n",
              "      <td>3.3696</td>\n",
              "      <td>-17.1855</td>\n",
              "      <td>2.8292</td>\n",
              "      <td>5.2606</td>\n",
              "      <td>2.6836</td>\n",
              "      <td>5.8767</td>\n",
              "      <td>25.1262</td>\n",
              "      <td>7.3478</td>\n",
              "      <td>27.1264</td>\n",
              "      <td>11.8542</td>\n",
              "      <td>9.7999</td>\n",
              "      <td>11.1395</td>\n",
              "      <td>-3.2870</td>\n",
              "      <td>0.4285</td>\n",
              "      <td>2.5058</td>\n",
              "      <td>10.0339</td>\n",
              "      <td>9.1610</td>\n",
              "      <td>9.4318</td>\n",
              "      <td>13.4913</td>\n",
              "      <td>4.6247</td>\n",
              "      <td>6.2906</td>\n",
              "      <td>-17.8522</td>\n",
              "      <td>18.6751</td>\n",
              "      <td>-0.1162</td>\n",
              "      <td>4.9611</td>\n",
              "      <td>4.6549</td>\n",
              "      <td>0.6998</td>\n",
              "      <td>1.8341</td>\n",
              "      <td>22.2717</td>\n",
              "      <td>1.7337</td>\n",
              "      <td>-2.1651</td>\n",
              "      <td>6.7419</td>\n",
              "      <td>15.9054</td>\n",
              "      <td>0.3388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199997</th>\n",
              "      <td>train_199997</td>\n",
              "      <td>0</td>\n",
              "      <td>11.2232</td>\n",
              "      <td>-5.0518</td>\n",
              "      <td>10.5127</td>\n",
              "      <td>5.6456</td>\n",
              "      <td>9.3410</td>\n",
              "      <td>-5.4086</td>\n",
              "      <td>4.5555</td>\n",
              "      <td>21.5571</td>\n",
              "      <td>0.1202</td>\n",
              "      <td>6.1629</td>\n",
              "      <td>4.4004</td>\n",
              "      <td>-0.4651</td>\n",
              "      <td>13.8775</td>\n",
              "      <td>9.7414</td>\n",
              "      <td>10.9044</td>\n",
              "      <td>14.5597</td>\n",
              "      <td>9.6214</td>\n",
              "      <td>-1.6429</td>\n",
              "      <td>23.1127</td>\n",
              "      <td>12.1517</td>\n",
              "      <td>16.2577</td>\n",
              "      <td>3.1453</td>\n",
              "      <td>3.1008</td>\n",
              "      <td>2.1497</td>\n",
              "      <td>10.2715</td>\n",
              "      <td>13.5637</td>\n",
              "      <td>4.9473</td>\n",
              "      <td>-0.9905</td>\n",
              "      <td>6.2801</td>\n",
              "      <td>9.4902</td>\n",
              "      <td>-12.8549</td>\n",
              "      <td>11.0403</td>\n",
              "      <td>1.4306</td>\n",
              "      <td>13.8533</td>\n",
              "      <td>11.7484</td>\n",
              "      <td>6.8969</td>\n",
              "      <td>6.4162</td>\n",
              "      <td>3.4246</td>\n",
              "      <td>...</td>\n",
              "      <td>19.9293</td>\n",
              "      <td>5.3427</td>\n",
              "      <td>5.4776</td>\n",
              "      <td>13.1202</td>\n",
              "      <td>5.3500</td>\n",
              "      <td>31.7346</td>\n",
              "      <td>3.1693</td>\n",
              "      <td>-19.4779</td>\n",
              "      <td>6.8053</td>\n",
              "      <td>5.6281</td>\n",
              "      <td>-0.8774</td>\n",
              "      <td>-8.9508</td>\n",
              "      <td>17.4931</td>\n",
              "      <td>-1.6530</td>\n",
              "      <td>32.0032</td>\n",
              "      <td>12.5749</td>\n",
              "      <td>5.8756</td>\n",
              "      <td>8.8059</td>\n",
              "      <td>-10.6367</td>\n",
              "      <td>5.4401</td>\n",
              "      <td>-12.7967</td>\n",
              "      <td>8.7990</td>\n",
              "      <td>0.7021</td>\n",
              "      <td>14.9744</td>\n",
              "      <td>18.9211</td>\n",
              "      <td>0.3016</td>\n",
              "      <td>11.2869</td>\n",
              "      <td>-6.3741</td>\n",
              "      <td>12.9726</td>\n",
              "      <td>2.3425</td>\n",
              "      <td>4.0651</td>\n",
              "      <td>5.4414</td>\n",
              "      <td>3.1032</td>\n",
              "      <td>4.8793</td>\n",
              "      <td>23.5311</td>\n",
              "      <td>-1.5736</td>\n",
              "      <td>1.2832</td>\n",
              "      <td>8.7155</td>\n",
              "      <td>13.8329</td>\n",
              "      <td>4.1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>train_199998</td>\n",
              "      <td>0</td>\n",
              "      <td>9.7148</td>\n",
              "      <td>-8.6098</td>\n",
              "      <td>13.6104</td>\n",
              "      <td>5.7930</td>\n",
              "      <td>12.5173</td>\n",
              "      <td>0.5339</td>\n",
              "      <td>6.0479</td>\n",
              "      <td>17.0152</td>\n",
              "      <td>-2.1926</td>\n",
              "      <td>8.7542</td>\n",
              "      <td>1.4245</td>\n",
              "      <td>0.7086</td>\n",
              "      <td>14.2110</td>\n",
              "      <td>6.5641</td>\n",
              "      <td>7.6177</td>\n",
              "      <td>13.8771</td>\n",
              "      <td>9.0479</td>\n",
              "      <td>-11.8164</td>\n",
              "      <td>14.0831</td>\n",
              "      <td>-2.0345</td>\n",
              "      <td>18.3863</td>\n",
              "      <td>3.0911</td>\n",
              "      <td>5.5803</td>\n",
              "      <td>3.7091</td>\n",
              "      <td>12.8219</td>\n",
              "      <td>13.8866</td>\n",
              "      <td>-3.3859</td>\n",
              "      <td>-0.4440</td>\n",
              "      <td>5.4817</td>\n",
              "      <td>4.0902</td>\n",
              "      <td>-7.7085</td>\n",
              "      <td>10.3952</td>\n",
              "      <td>2.5739</td>\n",
              "      <td>17.8529</td>\n",
              "      <td>11.3433</td>\n",
              "      <td>5.0534</td>\n",
              "      <td>-3.0055</td>\n",
              "      <td>3.9433</td>\n",
              "      <td>...</td>\n",
              "      <td>40.3378</td>\n",
              "      <td>5.5357</td>\n",
              "      <td>4.6151</td>\n",
              "      <td>8.5910</td>\n",
              "      <td>-12.6998</td>\n",
              "      <td>25.8578</td>\n",
              "      <td>2.2346</td>\n",
              "      <td>-6.4988</td>\n",
              "      <td>2.6702</td>\n",
              "      <td>5.3868</td>\n",
              "      <td>-7.1875</td>\n",
              "      <td>8.1477</td>\n",
              "      <td>22.4362</td>\n",
              "      <td>-2.5914</td>\n",
              "      <td>8.8704</td>\n",
              "      <td>11.6621</td>\n",
              "      <td>7.4904</td>\n",
              "      <td>8.1808</td>\n",
              "      <td>-11.4177</td>\n",
              "      <td>2.8379</td>\n",
              "      <td>3.8748</td>\n",
              "      <td>8.7410</td>\n",
              "      <td>8.9998</td>\n",
              "      <td>16.4058</td>\n",
              "      <td>11.3244</td>\n",
              "      <td>-2.1751</td>\n",
              "      <td>12.4735</td>\n",
              "      <td>-18.3932</td>\n",
              "      <td>12.6337</td>\n",
              "      <td>0.3243</td>\n",
              "      <td>2.6840</td>\n",
              "      <td>8.6587</td>\n",
              "      <td>2.7337</td>\n",
              "      <td>11.1178</td>\n",
              "      <td>20.4158</td>\n",
              "      <td>-0.0786</td>\n",
              "      <td>6.7980</td>\n",
              "      <td>10.0342</td>\n",
              "      <td>15.5289</td>\n",
              "      <td>-13.9001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>train_199999</td>\n",
              "      <td>0</td>\n",
              "      <td>10.8762</td>\n",
              "      <td>-5.7105</td>\n",
              "      <td>12.1183</td>\n",
              "      <td>8.0328</td>\n",
              "      <td>11.5577</td>\n",
              "      <td>0.3488</td>\n",
              "      <td>5.2839</td>\n",
              "      <td>15.2058</td>\n",
              "      <td>-0.4541</td>\n",
              "      <td>9.3688</td>\n",
              "      <td>-7.3826</td>\n",
              "      <td>-8.7049</td>\n",
              "      <td>14.2486</td>\n",
              "      <td>15.0849</td>\n",
              "      <td>5.2313</td>\n",
              "      <td>14.3572</td>\n",
              "      <td>12.5523</td>\n",
              "      <td>-6.5066</td>\n",
              "      <td>11.3592</td>\n",
              "      <td>11.4779</td>\n",
              "      <td>15.4997</td>\n",
              "      <td>3.8474</td>\n",
              "      <td>2.4381</td>\n",
              "      <td>2.8295</td>\n",
              "      <td>10.6681</td>\n",
              "      <td>13.7167</td>\n",
              "      <td>-7.7771</td>\n",
              "      <td>-2.7798</td>\n",
              "      <td>6.2885</td>\n",
              "      <td>6.0089</td>\n",
              "      <td>2.1547</td>\n",
              "      <td>10.8181</td>\n",
              "      <td>-0.2712</td>\n",
              "      <td>12.5254</td>\n",
              "      <td>11.6304</td>\n",
              "      <td>-1.4949</td>\n",
              "      <td>7.9509</td>\n",
              "      <td>2.2480</td>\n",
              "      <td>...</td>\n",
              "      <td>15.9041</td>\n",
              "      <td>5.3187</td>\n",
              "      <td>6.2987</td>\n",
              "      <td>13.0729</td>\n",
              "      <td>-4.2045</td>\n",
              "      <td>19.2141</td>\n",
              "      <td>3.2902</td>\n",
              "      <td>-1.2175</td>\n",
              "      <td>4.1583</td>\n",
              "      <td>5.7675</td>\n",
              "      <td>5.7719</td>\n",
              "      <td>-1.2139</td>\n",
              "      <td>21.8496</td>\n",
              "      <td>-3.5368</td>\n",
              "      <td>25.9094</td>\n",
              "      <td>11.7673</td>\n",
              "      <td>1.9765</td>\n",
              "      <td>15.9218</td>\n",
              "      <td>3.9350</td>\n",
              "      <td>4.3993</td>\n",
              "      <td>-10.3268</td>\n",
              "      <td>10.5200</td>\n",
              "      <td>9.9587</td>\n",
              "      <td>11.9242</td>\n",
              "      <td>7.0626</td>\n",
              "      <td>-6.5429</td>\n",
              "      <td>10.5947</td>\n",
              "      <td>-3.8827</td>\n",
              "      <td>16.3552</td>\n",
              "      <td>1.7535</td>\n",
              "      <td>8.9842</td>\n",
              "      <td>1.6893</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.3766</td>\n",
              "      <td>15.2101</td>\n",
              "      <td>-2.4907</td>\n",
              "      <td>-2.2342</td>\n",
              "      <td>8.1857</td>\n",
              "      <td>12.1284</td>\n",
              "      <td>0.1385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows × 202 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             ID_code  target    var_0  ...  var_197  var_198  var_199\n",
              "0            train_0       0   8.9255  ...   8.5635  12.7803  -1.0914\n",
              "1            train_1       0  11.5006  ...   8.7889  18.3560   1.9518\n",
              "2            train_2       0   8.6093  ...   8.2675  14.7222   0.3965\n",
              "3            train_3       0  11.0604  ...  10.2922  17.9697  -8.9996\n",
              "4            train_4       0   9.8369  ...   9.5031  17.9974  -8.8104\n",
              "...              ...     ...      ...  ...      ...      ...      ...\n",
              "199995  train_199995       0  11.4880  ...   8.5326  16.6660 -17.8661\n",
              "199996  train_199996       0   4.9149  ...   6.7419  15.9054   0.3388\n",
              "199997  train_199997       0  11.2232  ...   8.7155  13.8329   4.1995\n",
              "199998  train_199998       0   9.7148  ...  10.0342  15.5289 -13.9001\n",
              "199999  train_199999       0  10.8762  ...   8.1857  12.1284   0.1385\n",
              "\n",
              "[200000 rows x 202 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Rfews8wdl9o1",
        "outputId": "dba8b854-7baa-4abe-a1da-6de7258890ed"
      },
      "source": [
        "data_agg = data.drop(columns='target').groupby('ID_code').agg(['mean', 'min', 'max', 'count', 'skew'])\n",
        "# data를 다 합쳐서 aggregation을 했는데 오버피팅의 위험이 있다.\n",
        "# 오버피팅 된다면 train, test나눠서 하자"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-569179108f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ID_code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'skew'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# data를 다 합쳐서 aggregation을 했는데 오버피팅의 위험이 있다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 오버피팅 된다면 train, test나눠서 하자\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;31m# we require a list, but not an 'str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_multiple_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_aggregate_multiple_funcs\u001b[0;34m(self, arg, _axis)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0mcolg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m                     \u001b[0mnew_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;31m# but not the class list / tuple itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_multiple_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrelabeling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_aggregate_multiple_funcs\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj_with_exclusions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                 if not re.search(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \"\"\"\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         return self._wrap_applied_output(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mcurried\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;31m# preserve the name so we can detect it when calling plot methods,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11473\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_by_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11474\u001b[0m         return self._reduce(\n\u001b[0;32m> 11475\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11476\u001b[0m         )\n\u001b[1;32m  11477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4247\u001b[0m                 )\n\u001b[1;32m   4248\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4249\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reindex_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanskew\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm3\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt7HilfknFc8"
      },
      "source": [
        "data_agg_col = []\n",
        "\n",
        "for i in data_agg.columns.levels[0]:\n",
        "  for j in data_agg.columns.levels[1]:\n",
        "    data_agg_col.append(f'{i}-{j}')\n",
        "\n",
        "data_agg.columns = data_agg_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MntR8bOOnWLO"
      },
      "source": [
        "corr_matrix = data_agg.corr()\n",
        "\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "to_drop = [c for c in upper.columns if any(abs(upper[c]) > 0.9)]\n",
        "\n",
        "print(f'There are {len(data_agg.columns)} columns but {len(to_drop)} corr columns to remove')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYLocOvzn6zR"
      },
      "source": [
        "data_agg = data_agg.drop(columns=to_drop)\n",
        "data = data.merge(data_agg, how='left', on='ID_code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtOznrSpq8bF"
      },
      "source": [
        "x_train = data.loc[data['target'].notnull()].drop(columns=['ID_code', 'target'])\n",
        "y_train = data.loc[data['target'].notnull()]['target']\n",
        "x_test = data.loc[data['target'].isnull()].drop(columns=['ID_code', 'target'])\n",
        "x_train_col = x_train.columns"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGkjZOBeveQ_"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "x_train = MinMaxScaler().fit_transform(x_train)\n",
        "x_test = MinMaxScaler().fit_transform(x_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erTHPk4lwB6r"
      },
      "source": [
        "x_train = pd.DataFrame(x_train, columns=x_train_col)\n",
        "x_test = pd.DataFrame(x_test, columns=x_train_col)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc2aVZqfwTfq",
        "outputId": "9043ad10-47f6-4241-a972-4c14ca5245b4"
      },
      "source": [
        "pip install catboost"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (0.26)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDbKgFnjwYK2"
      },
      "source": [
        "import catboost\n",
        "import lightgbm\n",
        "import xgboost\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "from sklearn.model_selection import cross_validate, KFold, StratifiedKFold"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCHnI3Xn0mIw",
        "outputId": "3e9e899d-5268-4a14-fe2d-ef682d42eb78"
      },
      "source": [
        "auc={}\n",
        "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for i, (train_index, valid_index) in enumerate(skf.split(x_train, y_train)):\n",
        "  X_train, X_valid = x_train.iloc[train_index], x_train.iloc[valid_index]\n",
        "  Y_train, Y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
        "\n",
        "  model = lightgbm.LGBMClassifier(n_estimators=3000, learning_rate=0.022, objective='cross_entropy', boosting_type='goss')\n",
        "  model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], verbose=100, early_stopping_rounds=70, eval_metric='auc')\n",
        "  preds_model = model.predict_proba(x_test)[:,1]\n",
        "  preds_model_valid = model.predict_proba(X_valid)[:,1]\n",
        "  auc[i] = roc_auc_score(Y_valid, preds_model_valid)\n",
        "  print(f'{i}번째 AUC값:{auc[i]}')\n",
        "print(f'mean AUC값:{auc.values}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 70 rounds.\n",
            "[100]\ttraining's auc: 0.855867\ttraining's xentropy: 0.27317\tvalid_1's auc: 0.818172\tvalid_1's xentropy: 0.282369\n",
            "[200]\ttraining's auc: 0.893564\ttraining's xentropy: 0.24532\tvalid_1's auc: 0.848965\tvalid_1's xentropy: 0.261498\n",
            "[300]\ttraining's auc: 0.9137\ttraining's xentropy: 0.226001\tvalid_1's auc: 0.864251\tvalid_1's xentropy: 0.248305\n",
            "[400]\ttraining's auc: 0.927175\ttraining's xentropy: 0.211138\tvalid_1's auc: 0.873093\tvalid_1's xentropy: 0.239092\n",
            "[500]\ttraining's auc: 0.936748\ttraining's xentropy: 0.199091\tvalid_1's auc: 0.879585\tvalid_1's xentropy: 0.23199\n",
            "[600]\ttraining's auc: 0.944212\ttraining's xentropy: 0.188921\tvalid_1's auc: 0.883481\tvalid_1's xentropy: 0.226708\n",
            "[700]\ttraining's auc: 0.950524\ttraining's xentropy: 0.180108\tvalid_1's auc: 0.886592\tvalid_1's xentropy: 0.222487\n",
            "[800]\ttraining's auc: 0.955881\ttraining's xentropy: 0.172262\tvalid_1's auc: 0.888974\tvalid_1's xentropy: 0.219075\n",
            "[900]\ttraining's auc: 0.960377\ttraining's xentropy: 0.165299\tvalid_1's auc: 0.890861\tvalid_1's xentropy: 0.216267\n",
            "[1000]\ttraining's auc: 0.964268\ttraining's xentropy: 0.158944\tvalid_1's auc: 0.892228\tvalid_1's xentropy: 0.213937\n",
            "[1100]\ttraining's auc: 0.967791\ttraining's xentropy: 0.15313\tvalid_1's auc: 0.89349\tvalid_1's xentropy: 0.212057\n",
            "[1200]\ttraining's auc: 0.971037\ttraining's xentropy: 0.147701\tvalid_1's auc: 0.894389\tvalid_1's xentropy: 0.210564\n",
            "[1300]\ttraining's auc: 0.974002\ttraining's xentropy: 0.142628\tvalid_1's auc: 0.895054\tvalid_1's xentropy: 0.20936\n",
            "[1400]\ttraining's auc: 0.976725\ttraining's xentropy: 0.137916\tvalid_1's auc: 0.895488\tvalid_1's xentropy: 0.208458\n",
            "[1500]\ttraining's auc: 0.979196\ttraining's xentropy: 0.133498\tvalid_1's auc: 0.895754\tvalid_1's xentropy: 0.207785\n",
            "[1600]\ttraining's auc: 0.981483\ttraining's xentropy: 0.12929\tvalid_1's auc: 0.895909\tvalid_1's xentropy: 0.207272\n",
            "[1700]\ttraining's auc: 0.983535\ttraining's xentropy: 0.125326\tvalid_1's auc: 0.896096\tvalid_1's xentropy: 0.206865\n",
            "[1800]\ttraining's auc: 0.985421\ttraining's xentropy: 0.121553\tvalid_1's auc: 0.896229\tvalid_1's xentropy: 0.2065\n",
            "[1900]\ttraining's auc: 0.987154\ttraining's xentropy: 0.117952\tvalid_1's auc: 0.896444\tvalid_1's xentropy: 0.206184\n",
            "Early stopping, best iteration is:\n",
            "[1903]\ttraining's auc: 0.987198\ttraining's xentropy: 0.11785\tvalid_1's auc: 0.896445\tvalid_1's xentropy: 0.206179\n",
            "0번째 AUC값:0.8964448067643231\n",
            "Training until validation scores don't improve for 70 rounds.\n",
            "[100]\ttraining's auc: 0.855114\ttraining's xentropy: 0.273419\tvalid_1's auc: 0.823896\tvalid_1's xentropy: 0.28192\n",
            "[200]\ttraining's auc: 0.893528\ttraining's xentropy: 0.245589\tvalid_1's auc: 0.85233\tvalid_1's xentropy: 0.261442\n",
            "[300]\ttraining's auc: 0.91361\ttraining's xentropy: 0.226315\tvalid_1's auc: 0.866475\tvalid_1's xentropy: 0.248188\n",
            "[400]\ttraining's auc: 0.926943\ttraining's xentropy: 0.211408\tvalid_1's auc: 0.874956\tvalid_1's xentropy: 0.239039\n",
            "[500]\ttraining's auc: 0.936692\ttraining's xentropy: 0.19923\tvalid_1's auc: 0.880741\tvalid_1's xentropy: 0.231976\n",
            "[600]\ttraining's auc: 0.944311\ttraining's xentropy: 0.188975\tvalid_1's auc: 0.884963\tvalid_1's xentropy: 0.226518\n",
            "[700]\ttraining's auc: 0.950403\ttraining's xentropy: 0.180123\tvalid_1's auc: 0.887661\tvalid_1's xentropy: 0.222247\n",
            "[800]\ttraining's auc: 0.955601\ttraining's xentropy: 0.172268\tvalid_1's auc: 0.88965\tvalid_1's xentropy: 0.218837\n",
            "[900]\ttraining's auc: 0.960108\ttraining's xentropy: 0.165256\tvalid_1's auc: 0.891288\tvalid_1's xentropy: 0.216077\n",
            "[1000]\ttraining's auc: 0.964081\ttraining's xentropy: 0.158919\tvalid_1's auc: 0.892522\tvalid_1's xentropy: 0.213837\n",
            "[1100]\ttraining's auc: 0.967558\ttraining's xentropy: 0.153128\tvalid_1's auc: 0.89362\tvalid_1's xentropy: 0.211902\n",
            "[1200]\ttraining's auc: 0.970801\ttraining's xentropy: 0.147742\tvalid_1's auc: 0.894281\tvalid_1's xentropy: 0.210476\n",
            "[1300]\ttraining's auc: 0.973683\ttraining's xentropy: 0.142725\tvalid_1's auc: 0.894879\tvalid_1's xentropy: 0.209309\n",
            "[1400]\ttraining's auc: 0.976387\ttraining's xentropy: 0.138023\tvalid_1's auc: 0.895522\tvalid_1's xentropy: 0.208243\n",
            "[1500]\ttraining's auc: 0.978802\ttraining's xentropy: 0.133612\tvalid_1's auc: 0.895833\tvalid_1's xentropy: 0.207498\n",
            "[1600]\ttraining's auc: 0.981184\ttraining's xentropy: 0.12939\tvalid_1's auc: 0.89598\tvalid_1's xentropy: 0.206968\n",
            "[1700]\ttraining's auc: 0.983347\ttraining's xentropy: 0.125414\tvalid_1's auc: 0.896276\tvalid_1's xentropy: 0.206406\n",
            "[1800]\ttraining's auc: 0.985278\ttraining's xentropy: 0.121627\tvalid_1's auc: 0.896395\tvalid_1's xentropy: 0.206048\n",
            "Early stopping, best iteration is:\n",
            "[1754]\ttraining's auc: 0.984384\ttraining's xentropy: 0.123359\tvalid_1's auc: 0.896413\tvalid_1's xentropy: 0.206143\n",
            "1번째 AUC값:0.8964134391268224\n",
            "Training until validation scores don't improve for 70 rounds.\n",
            "[100]\ttraining's auc: 0.857217\ttraining's xentropy: 0.272777\tvalid_1's auc: 0.811381\tvalid_1's xentropy: 0.283909\n",
            "[200]\ttraining's auc: 0.894989\ttraining's xentropy: 0.244731\tvalid_1's auc: 0.841802\tvalid_1's xentropy: 0.263704\n",
            "[300]\ttraining's auc: 0.914995\ttraining's xentropy: 0.225291\tvalid_1's auc: 0.855927\tvalid_1's xentropy: 0.2511\n",
            "[400]\ttraining's auc: 0.927994\ttraining's xentropy: 0.210464\tvalid_1's auc: 0.864773\tvalid_1's xentropy: 0.242138\n",
            "[500]\ttraining's auc: 0.937413\ttraining's xentropy: 0.198317\tvalid_1's auc: 0.870771\tvalid_1's xentropy: 0.235422\n",
            "[600]\ttraining's auc: 0.945012\ttraining's xentropy: 0.188148\tvalid_1's auc: 0.875264\tvalid_1's xentropy: 0.230101\n",
            "[700]\ttraining's auc: 0.951038\ttraining's xentropy: 0.179346\tvalid_1's auc: 0.878691\tvalid_1's xentropy: 0.225905\n",
            "[800]\ttraining's auc: 0.956245\ttraining's xentropy: 0.171588\tvalid_1's auc: 0.88109\tvalid_1's xentropy: 0.222589\n",
            "[900]\ttraining's auc: 0.96057\ttraining's xentropy: 0.164626\tvalid_1's auc: 0.883482\tvalid_1's xentropy: 0.21965\n",
            "[1000]\ttraining's auc: 0.964522\ttraining's xentropy: 0.158287\tvalid_1's auc: 0.884912\tvalid_1's xentropy: 0.21742\n",
            "[1100]\ttraining's auc: 0.968049\ttraining's xentropy: 0.152457\tvalid_1's auc: 0.886315\tvalid_1's xentropy: 0.215493\n",
            "[1200]\ttraining's auc: 0.971203\ttraining's xentropy: 0.147054\tvalid_1's auc: 0.887266\tvalid_1's xentropy: 0.21409\n",
            "[1300]\ttraining's auc: 0.97412\ttraining's xentropy: 0.142037\tvalid_1's auc: 0.888014\tvalid_1's xentropy: 0.212941\n",
            "[1400]\ttraining's auc: 0.976682\ttraining's xentropy: 0.137373\tvalid_1's auc: 0.888673\tvalid_1's xentropy: 0.211963\n",
            "[1500]\ttraining's auc: 0.979155\ttraining's xentropy: 0.132961\tvalid_1's auc: 0.88923\tvalid_1's xentropy: 0.211195\n",
            "[1600]\ttraining's auc: 0.981399\ttraining's xentropy: 0.128764\tvalid_1's auc: 0.889538\tvalid_1's xentropy: 0.210637\n",
            "[1700]\ttraining's auc: 0.983538\ttraining's xentropy: 0.124777\tvalid_1's auc: 0.889796\tvalid_1's xentropy: 0.210222\n",
            "Early stopping, best iteration is:\n",
            "[1697]\ttraining's auc: 0.983464\ttraining's xentropy: 0.124897\tvalid_1's auc: 0.889812\tvalid_1's xentropy: 0.210222\n",
            "2번째 AUC값:0.889812471826526\n",
            "Training until validation scores don't improve for 70 rounds.\n",
            "[100]\ttraining's auc: 0.856195\ttraining's xentropy: 0.272886\tvalid_1's auc: 0.818401\tvalid_1's xentropy: 0.282679\n",
            "[200]\ttraining's auc: 0.893851\ttraining's xentropy: 0.245049\tvalid_1's auc: 0.848415\tvalid_1's xentropy: 0.262252\n",
            "[300]\ttraining's auc: 0.91375\ttraining's xentropy: 0.225764\tvalid_1's auc: 0.862728\tvalid_1's xentropy: 0.249194\n",
            "[400]\ttraining's auc: 0.927418\ttraining's xentropy: 0.210938\tvalid_1's auc: 0.871937\tvalid_1's xentropy: 0.239831\n",
            "[500]\ttraining's auc: 0.937035\ttraining's xentropy: 0.198854\tvalid_1's auc: 0.877965\tvalid_1's xentropy: 0.232794\n",
            "[600]\ttraining's auc: 0.944579\ttraining's xentropy: 0.188634\tvalid_1's auc: 0.8821\tvalid_1's xentropy: 0.227462\n",
            "[700]\ttraining's auc: 0.950647\ttraining's xentropy: 0.179807\tvalid_1's auc: 0.884919\tvalid_1's xentropy: 0.22326\n",
            "[800]\ttraining's auc: 0.955859\ttraining's xentropy: 0.172006\tvalid_1's auc: 0.887283\tvalid_1's xentropy: 0.219888\n",
            "[900]\ttraining's auc: 0.960358\ttraining's xentropy: 0.164981\tvalid_1's auc: 0.889183\tvalid_1's xentropy: 0.21706\n",
            "[1000]\ttraining's auc: 0.964389\ttraining's xentropy: 0.158628\tvalid_1's auc: 0.890534\tvalid_1's xentropy: 0.214877\n",
            "[1100]\ttraining's auc: 0.96785\ttraining's xentropy: 0.152823\tvalid_1's auc: 0.891641\tvalid_1's xentropy: 0.212998\n",
            "[1200]\ttraining's auc: 0.971025\ttraining's xentropy: 0.147417\tvalid_1's auc: 0.892397\tvalid_1's xentropy: 0.211595\n",
            "[1300]\ttraining's auc: 0.973903\ttraining's xentropy: 0.142435\tvalid_1's auc: 0.893039\tvalid_1's xentropy: 0.210444\n",
            "[1400]\ttraining's auc: 0.976621\ttraining's xentropy: 0.137712\tvalid_1's auc: 0.893534\tvalid_1's xentropy: 0.209469\n",
            "[1500]\ttraining's auc: 0.979039\ttraining's xentropy: 0.133268\tvalid_1's auc: 0.89387\tvalid_1's xentropy: 0.208791\n",
            "[1600]\ttraining's auc: 0.981374\ttraining's xentropy: 0.129069\tvalid_1's auc: 0.894251\tvalid_1's xentropy: 0.208208\n",
            "[1700]\ttraining's auc: 0.983411\ttraining's xentropy: 0.12513\tvalid_1's auc: 0.894498\tvalid_1's xentropy: 0.207799\n",
            "[1800]\ttraining's auc: 0.98536\ttraining's xentropy: 0.121348\tvalid_1's auc: 0.894588\tvalid_1's xentropy: 0.207525\n",
            "[1900]\ttraining's auc: 0.987031\ttraining's xentropy: 0.11776\tvalid_1's auc: 0.894684\tvalid_1's xentropy: 0.207235\n",
            "Early stopping, best iteration is:\n",
            "[1919]\ttraining's auc: 0.987332\ttraining's xentropy: 0.11709\tvalid_1's auc: 0.894745\tvalid_1's xentropy: 0.207174\n",
            "3번째 AUC값:0.8947449453676587\n",
            "Training until validation scores don't improve for 70 rounds.\n",
            "[100]\ttraining's auc: 0.857179\ttraining's xentropy: 0.272931\tvalid_1's auc: 0.819587\tvalid_1's xentropy: 0.283162\n",
            "[200]\ttraining's auc: 0.894592\ttraining's xentropy: 0.24493\tvalid_1's auc: 0.849931\tvalid_1's xentropy: 0.26273\n",
            "[300]\ttraining's auc: 0.914066\ttraining's xentropy: 0.22555\tvalid_1's auc: 0.864219\tvalid_1's xentropy: 0.249568\n",
            "[400]\ttraining's auc: 0.927417\ttraining's xentropy: 0.21066\tvalid_1's auc: 0.873954\tvalid_1's xentropy: 0.2401\n",
            "[500]\ttraining's auc: 0.936872\ttraining's xentropy: 0.198622\tvalid_1's auc: 0.879982\tvalid_1's xentropy: 0.233152\n",
            "[600]\ttraining's auc: 0.944496\ttraining's xentropy: 0.18842\tvalid_1's auc: 0.883939\tvalid_1's xentropy: 0.22781\n",
            "[700]\ttraining's auc: 0.950678\ttraining's xentropy: 0.179576\tvalid_1's auc: 0.887273\tvalid_1's xentropy: 0.22349\n",
            "[800]\ttraining's auc: 0.95591\ttraining's xentropy: 0.17178\tvalid_1's auc: 0.889504\tvalid_1's xentropy: 0.220174\n",
            "[900]\ttraining's auc: 0.960363\ttraining's xentropy: 0.164819\tvalid_1's auc: 0.891434\tvalid_1's xentropy: 0.217348\n",
            "[1000]\ttraining's auc: 0.964185\ttraining's xentropy: 0.158461\tvalid_1's auc: 0.892602\tvalid_1's xentropy: 0.215281\n",
            "[1100]\ttraining's auc: 0.967707\ttraining's xentropy: 0.152637\tvalid_1's auc: 0.893619\tvalid_1's xentropy: 0.213506\n",
            "[1200]\ttraining's auc: 0.970873\ttraining's xentropy: 0.147271\tvalid_1's auc: 0.894649\tvalid_1's xentropy: 0.211937\n",
            "[1300]\ttraining's auc: 0.973697\ttraining's xentropy: 0.142288\tvalid_1's auc: 0.895361\tvalid_1's xentropy: 0.210722\n",
            "[1400]\ttraining's auc: 0.976354\ttraining's xentropy: 0.137603\tvalid_1's auc: 0.895861\tvalid_1's xentropy: 0.209784\n",
            "[1500]\ttraining's auc: 0.978708\ttraining's xentropy: 0.1332\tvalid_1's auc: 0.896333\tvalid_1's xentropy: 0.20894\n",
            "[1600]\ttraining's auc: 0.980999\ttraining's xentropy: 0.129004\tvalid_1's auc: 0.896413\tvalid_1's xentropy: 0.208425\n",
            "[1700]\ttraining's auc: 0.983124\ttraining's xentropy: 0.125064\tvalid_1's auc: 0.896586\tvalid_1's xentropy: 0.207973\n",
            "[1800]\ttraining's auc: 0.984958\ttraining's xentropy: 0.121297\tvalid_1's auc: 0.896693\tvalid_1's xentropy: 0.207647\n",
            "[1900]\ttraining's auc: 0.98674\ttraining's xentropy: 0.117739\tvalid_1's auc: 0.896811\tvalid_1's xentropy: 0.207359\n",
            "[2000]\ttraining's auc: 0.988345\ttraining's xentropy: 0.114292\tvalid_1's auc: 0.897085\tvalid_1's xentropy: 0.207028\n",
            "[2100]\ttraining's auc: 0.989812\ttraining's xentropy: 0.110996\tvalid_1's auc: 0.897282\tvalid_1's xentropy: 0.206782\n",
            "[2200]\ttraining's auc: 0.991085\ttraining's xentropy: 0.107914\tvalid_1's auc: 0.897298\tvalid_1's xentropy: 0.2067\n",
            "Early stopping, best iteration is:\n",
            "[2144]\ttraining's auc: 0.990366\ttraining's xentropy: 0.109623\tvalid_1's auc: 0.897387\tvalid_1's xentropy: 0.206671\n",
            "4번째 AUC값:0.8973867944878167\n",
            "mean AUC값:<built-in method values of dict object at 0x7f963fe5a780>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UbJHUawwz-N"
      },
      "source": [
        "def model(model, n_splits):\n",
        "  auc={}\n",
        "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "  for i, (train_index, valid_index) in enumerate(kf.split(x_train, y_train)):\n",
        "    X_train, X_valid = x_train.iloc[train_index], x_train.iloc[valid_index]\n",
        "    Y_train, Y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
        "\n",
        "    model = model\n",
        "    model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], verbose=100, early_stopping_rounds=70, eval_metric='auc')\n",
        "    preds_model = model.predict(x_test)[:,1]\n",
        "    preds_model_valid = model.predict_proba(X_valid)[:,1]\n",
        "    auc[i] = roc_auc_score(Y_valid, preds_model_valid)\n",
        "    print(f'{i}번째 AUC값:{auc[i]}')\n",
        "  print(f'mean AUC값:{auc.values}')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "fLLpqD5Xz_MJ",
        "outputId": "f534d907-55c9-40f0-fe91-44f3e49ec4ee"
      },
      "source": [
        "model(lightgbm.LGBMRegressor(n_estimators=3000, learning_rate=0.05, objective='auc'), 7)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-70e85804e3ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlightgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "H-v6O7aqBPJS",
        "outputId": "9e55a4e2-0654-4814-da6a-e5ae94d871f2"
      },
      "source": [
        "submit['target'] = preds_model\n",
        "submit"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_code</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test_0</td>\n",
              "      <td>0.233199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test_1</td>\n",
              "      <td>0.197793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test_2</td>\n",
              "      <td>0.317558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test_3</td>\n",
              "      <td>0.169651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test_4</td>\n",
              "      <td>0.093081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199995</th>\n",
              "      <td>test_199995</td>\n",
              "      <td>0.027904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199996</th>\n",
              "      <td>test_199996</td>\n",
              "      <td>0.005811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199997</th>\n",
              "      <td>test_199997</td>\n",
              "      <td>0.003665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>test_199998</td>\n",
              "      <td>0.046145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>test_199999</td>\n",
              "      <td>0.083718</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            ID_code    target\n",
              "0            test_0  0.233199\n",
              "1            test_1  0.197793\n",
              "2            test_2  0.317558\n",
              "3            test_3  0.169651\n",
              "4            test_4  0.093081\n",
              "...             ...       ...\n",
              "199995  test_199995  0.027904\n",
              "199996  test_199996  0.005811\n",
              "199997  test_199997  0.003665\n",
              "199998  test_199998  0.046145\n",
              "199999  test_199999  0.083718\n",
              "\n",
              "[200000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96WQLsc4BWUd"
      },
      "source": [
        "submit.to_csv('/content/drive/MyDrive/dataset/kaggle/submit_1.csv', index=False)"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}