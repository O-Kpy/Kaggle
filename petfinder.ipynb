{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-09T06:25:31.984803Z","iopub.execute_input":"2021-11-09T06:25:31.985567Z","iopub.status.idle":"2021-11-09T06:25:41.019948Z","shell.execute_reply.started":"2021-11-09T06:25:31.985476Z","shell.execute_reply":"2021-11-09T06:25:41.019126Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport copy\nimport cv2\nimport time\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torchvision.transforms as T\n\nimport timm\n\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom colorama import Fore, Back, Style\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","metadata":{"execution":{"iopub.status.busy":"2021-11-09T06:30:53.494970Z","iopub.execute_input":"2021-11-09T06:30:53.495294Z","iopub.status.idle":"2021-11-09T06:30:59.705654Z","shell.execute_reply.started":"2021-11-09T06:30:53.495262Z","shell.execute_reply":"2021-11-09T06:30:59.704785Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret('wandb_api')  # api 얻어오기\n    wandb.login(key=api_key)  # wandb 로그인\n    anony = None\nexcept:\n    anony = 'must'","metadata":{"execution":{"iopub.status.busy":"2021-11-09T06:40:46.797205Z","iopub.execute_input":"2021-11-09T06:40:46.797868Z","iopub.status.idle":"2021-11-09T06:40:47.010094Z","shell.execute_reply.started":"2021-11-09T06:40:46.797832Z","shell.execute_reply":"2021-11-09T06:40:47.009399Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = '../input/petfinder-pawpularity-score'\nTRAIN_DIR = '../input/petfinder-pawpularity-score/train'\nTEST_DIR = '../input/petfinder-pawpularity-score/test'","metadata":{"execution":{"iopub.status.busy":"2021-11-09T06:47:31.398062Z","iopub.execute_input":"2021-11-09T06:47:31.398552Z","iopub.status.idle":"2021-11-09T06:47:31.402791Z","shell.execute_reply.started":"2021-11-09T06:47:31.398517Z","shell.execute_reply":"2021-11-09T06:47:31.402040Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    seed = 42,\n    model_name1 = 'tf_efficientnet_b4_ns',\n    model_name2 = 'swin_large_patch4_window12_384',\n    train_batch_size = 16,\n    valid_batch_size = 32,\n    img_size = 512,\n    epochs = 10,\n    learning_rate = 1e-4,\n    min_lr = 1e-6,\n    T_max = 100,\n    T_0 = 25,\n    warmup_epochs = 0,\n    weight_decay = 1e-6,\n    n_accumulate = 1,\n    n_fold = 5,\n    num_classes = 1,\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n    competition = 'PetFinder',\n    wandb_kernel = 'deb'\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T06:52:59.613803Z","iopub.execute_input":"2021-11-09T06:52:59.614533Z","iopub.status.idle":"2021-11-09T06:52:59.671330Z","shell.execute_reply.started":"2021-11-09T06:52:59.614496Z","shell.execute_reply":"2021-11-09T06:52:59.670463Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)  # 해시 seed\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T06:55:19.295225Z","iopub.execute_input":"2021-11-09T06:55:19.296057Z","iopub.status.idle":"2021-11-09T06:55:19.304453Z","shell.execute_reply.started":"2021-11-09T06:55:19.296018Z","shell.execute_reply":"2021-11-09T06:55:19.303586Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_train_file_path(id):\n    return f'{TRAIN_DIR}/{id}.jpg'\n\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['file_path'] = df['Id'].apply(get_train_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T06:56:59.160119Z","iopub.execute_input":"2021-11-09T06:56:59.160401Z","iopub.status.idle":"2021-11-09T06:56:59.208445Z","shell.execute_reply.started":"2021-11-09T06:56:59.160372Z","shell.execute_reply":"2021-11-09T06:56:59.207776Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# KFOLD","metadata":{}},{"cell_type":"code","source":"def create_folds(df, n_s, n_grp=None):\n    df['kfold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s, random_state=CONFIG['seed'])\n        target = df['Pawpularity']\n    else:\n        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG['seed'])\n        df['grp'] = pd.cut(df['Pawpularity'], bins=n_grp, labels=False)\n        target = df['grp']\n    \n    for fold_no, (t,v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'kfold'] = fold_no\n        \n    df = df.drop(columns=['grp'])\n    return df\n\ndf = create_folds(df, n_s=CONFIG['n_fold'], n_grp=14)\nfeature_cols = [cols for cols in df.columns if cols not in ['Id', 'file_path', 'Pawpularity', 'kfold']]","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:04:05.437972Z","iopub.execute_input":"2021-11-09T07:04:05.438821Z","iopub.status.idle":"2021-11-09T07:04:05.496668Z","shell.execute_reply.started":"2021-11-09T07:04:05.438774Z","shell.execute_reply":"2021-11-09T07:04:05.495859Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:04:09.020031Z","iopub.execute_input":"2021-11-09T07:04:09.020715Z","iopub.status.idle":"2021-11-09T07:04:09.045951Z","shell.execute_reply.started":"2021-11-09T07:04:09.020662Z","shell.execute_reply":"2021-11-09T07:04:09.045146Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class PawpularityDataset(Dataset):\n    def __init__(self, root_dir, df, transforms=None):\n        self.root_dir = root_dir\n        self.df = df\n        self.file_name = df['file_path'].values\n        self.targets = df['Pawpularity'].values\n        self.meta = df[feature_cols].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_name[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        meta = self.meta[index, :]\n        targets = self.targets[index]\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n        return img, meta, targets","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:55:23.943687Z","iopub.execute_input":"2021-11-09T07:55:23.943946Z","iopub.status.idle":"2021-11-09T07:55:23.951994Z","shell.execute_reply.started":"2021-11-09T07:55:23.943917Z","shell.execute_reply":"2021-11-09T07:55:23.951295Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"MEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\ndata_transforms = {\n    'train' : A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0, p=1.0),\n        ToTensorV2()\n    ], p=1.0),\n    \n    'valid' : A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0, p=1.0),\n        ToTensorV2()\n    ], p=1.0)\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:13:08.803102Z","iopub.execute_input":"2021-11-09T07:13:08.803579Z","iopub.status.idle":"2021-11-09T07:13:08.811128Z","shell.execute_reply.started":"2021-11-09T07:13:08.803546Z","shell.execute_reply":"2021-11-09T07:13:08.810231Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# model 불러오기","metadata":{}},{"cell_type":"code","source":"class PawpularityModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super(PawpularityModel, self).__init__()\n        self.model = timm.create_model(model_name=model_name, pretrained=pretrained)\n        self.in_features = self.model.classifier.in_features\n        self.model.reset_classifier(0)\n        self.fc1 = nn.Linear(self.in_features+12, CONFIG['num_classes'])\n        self.dropout = nn.Dropout(p=0.2)\n        \n    def forward(self, images, meta):\n        features = self.model(images)\n        features = self.dropout(features)\n        features = torch.cat([features, meta], dim=1)\n        output = self.fc1(features)\n        return output\n\nmodel = PawpularityModel(CONFIG['model_name1'])\nmodel.to(CONFIG['device'])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:18:43.129127Z","iopub.execute_input":"2021-11-09T07:18:43.129803Z","iopub.status.idle":"2021-11-09T07:18:57.471711Z","shell.execute_reply.started":"2021-11-09T07:18:43.129768Z","shell.execute_reply":"2021-11-09T07:18:57.471026Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class PawpularityModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super(PawpularityModel, self).__init__()\n        self.model = timm.create_model(model_name=model_name, pretrained=pretrained)\n        self.n_features = self.model.classifier.in_features\n        self.model.reset_classifier(0)\n        self.fc = nn.Linear(self.n_features+12, CONFIG['num_classes'])\n        # self.fc2 = nn.Linear(512, CONFIG['num_classes'])\n        self.dropout = nn.Dropout(p=0.6)\n        \n    def forward(self, images, meta):\n        features = self.model(images)\n        features = self.dropout(features)\n        features = torch.cat([features, meta], dim=1)\n        output = self.fc(features)\n        return output\n    \nmodel = PawpularityModel(CONFIG['model_name'])\nmodel.to(CONFIG['device'])","metadata":{"execution":{"iopub.status.busy":"2021-10-19T08:32:54.127246Z","iopub.execute_input":"2021-10-19T08:32:54.127694Z","iopub.status.idle":"2021-10-19T08:33:02.532989Z","shell.execute_reply.started":"2021-10-19T08:32:54.127661Z","shell.execute_reply":"2021-10-19T08:33:02.532236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dummy run to initialize the layers \nimg = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).to(CONFIG['device'])\nmeta = torch.randn(1, len(feature_cols)).to(CONFIG['device'])\nmodel(img, meta)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# criterion\ndef criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:25:44.849992Z","iopub.execute_input":"2021-11-09T07:25:44.850310Z","iopub.status.idle":"2021-11-09T07:25:44.856676Z","shell.execute_reply.started":"2021-11-09T07:25:44.850277Z","shell.execute_reply":"2021-11-09T07:25:44.854383Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:\n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(images, meta)\n            loss = criterion(outputs, targets)\n            loss = loss / CONFIG['n_accumulate']\n            \n        scaler.scale(loss).backward()\n        \n        if (step+1) % CONFIG['n_accumulate'] == 0:  # freeze\n            scaler.step(optimizer)\n            scaler.update()\n            \n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n            \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n        \n    gc.collect()\n        \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:32:46.600929Z","iopub.execute_input":"2021-11-09T07:32:46.601210Z","iopub.status.idle":"2021-11-09T07:32:46.611816Z","shell.execute_reply.started":"2021-11-09T07:32:46.601165Z","shell.execute_reply":"2021-11-09T07:32:46.611155Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:         \n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(images, meta)\n            loss = criterion(outputs, targets)\n            loss = loss / CONFIG['n_accumulate']\n            \n        scaler.scale(loss).backward()  # .backward() == 기울기 계산\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            scaler.step(optimizer)  # .step() == parameter를 업데이트\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()  # .zero_grad() == 기울기 초기화 \n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-02T08:25:57.610133Z","iopub.execute_input":"2021-11-02T08:25:57.610399Z","iopub.status.idle":"2021-11-02T08:25:57.627553Z","shell.execute_reply.started":"2021-11-02T08:25:57.610369Z","shell.execute_reply":"2021-11-02T08:25:57.626664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    TARGETS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:\n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images, meta)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n    \n    PREDS = np.concatenate(PREDS)\n    TARGETS = np.concatenate(TARGETS)\n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    \n    gc.collect()\n    return epoch_loss, val_rmse","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:40:14.111559Z","iopub.execute_input":"2021-11-09T07:40:14.111830Z","iopub.status.idle":"2021-11-09T07:40:14.122577Z","shell.execute_reply.started":"2021-11-09T07:40:14.111800Z","shell.execute_reply":"2021-11-09T07:40:14.121903Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    TARGETS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, meta, targets) in bar:\n        images = images.to(device, dtype=torch.float)\n        meta = meta.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images, meta)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n        \n    TARGETS = np.concatenate(TARGETS)\n    PREDS = np.concatenate(PREDS)\n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    \n    gc.collect()\n    \n    return epoch_loss, val_rmse","metadata":{"execution":{"iopub.status.busy":"2021-11-02T08:25:59.659115Z","iopub.execute_input":"2021-11-02T08:25:59.659396Z","iopub.status.idle":"2021-11-02T08:25:59.670822Z","shell.execute_reply.started":"2021-11-02T08:25:59.659367Z","shell.execute_reply":"2021-11-02T08:25:59.670035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    wandb.watch(model, log_freq=100)\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs+1):\n        gc.collect()\n        \n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader=train_loader, device=CONFIG['device'], epoch=epoch)\n        valid_epoch_loss, valid_epoch_rmse = valid_one_epoch(model, dataloader=valid_loader, device=CONFIG['device'], epoch=epoch)\n        \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n        history['Valid RMSE'].append(valid_epoch_rmse)\n        \n        wandb.log({'Train Loss' : train_epoch_loss})\n        wandb.log({'Valid Loss' : valid_epoch_loss})\n        wandb.log({'Valid RMSE' : valid_epoch_rmse})\n        \n        if valid_epoch_rmse <= best_epoch_rmse:\n            print(f'{c_}Validation RMSE Improved({best_epoch_rmse} --> {valid_epoch_rmse})')\n            best_epoch_rmse = valid_epoch_rmse\n            run.summary['Best RMSE'] = best_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f'epoch:{epoch}.bin'\n            torch.save(model, PATH)\n            wandb.save(PATH)\n            print(f'Model Saved')\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n    \n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:48:49.837662Z","iopub.execute_input":"2021-11-09T07:48:49.838394Z","iopub.status.idle":"2021-11-09T07:48:49.851499Z","shell.execute_reply.started":"2021-11-09T07:48:49.838352Z","shell.execute_reply":"2021-11-09T07:48:49.850663Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    wandb.watch(model, log_freq=100)\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs+1):\n        gc.collect()\n        \n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader=train_loader, device=CONFIG['device'], epoch=epoch)\n        valid_epoch_loss, valid_epoch_rmse = valid_one_epoch(model, dataloader=valid_loader, device=CONFIG['device'], epoch=epoch)\n        \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n        history['Valid RMSE'].append(valid_epoch_rmse)\n        \n        wandb.log({'Train Loss' : train_epoch_loss})\n        wandb.log({'Valid Loss' : valid_epoch_loss})\n        wandb.log({'Valid RMSE' : valid_epoch_rmse})\n        \n        if valid_epoch_rmse <= best_epoch_rmse:\n            print(f'{c_}Validation RMSE Improved({best_epoch_rmse} --> {valid_epoch_rmse})')\n            best_epoch_rmse = valid_epoch_rmse\n            run.summary['Best RMSE'] = best_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f'epoch:{epoch}.bin'\n            torch.save(model, PATH)\n            wandb.save(PATH)\n            print(f'Model Saved{sr_}')\n            \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n    \n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-11-02T08:34:42.046731Z","iopub.execute_input":"2021-11-02T08:34:42.047385Z","iopub.status.idle":"2021-11-02T08:34:42.058605Z","shell.execute_reply.started":"2021-11-02T08:34:42.047346Z","shell.execute_reply":"2021-11-02T08:34:42.057888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['T_max'], eta_min=CONFIG['min_lr'])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:50:11.968056Z","iopub.execute_input":"2021-11-09T07:50:11.968328Z","iopub.status.idle":"2021-11-09T07:50:11.980358Z","shell.execute_reply.started":"2021-11-09T07:50:11.968299Z","shell.execute_reply":"2021-11-09T07:50:11.979626Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# optimizer, scheduler 선언\noptimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\nscheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], eta_min=CONFIG['min_lr'])","metadata":{"execution":{"iopub.status.busy":"2021-10-27T08:06:47.899344Z","iopub.execute_input":"2021-10-27T08:06:47.900019Z","iopub.status.idle":"2021-10-27T08:06:47.914291Z","shell.execute_reply.started":"2021-10-27T08:06:47.899983Z","shell.execute_reply":"2021-10-27T08:06:47.913531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 데이터 로더 준비","metadata":{}},{"cell_type":"code","source":"def prepared_loader(fold):\n    df_train = df[df['kfold'] != fold].reset_index(drop=True)\n    df_valid = df[df['kfold'] == fold].reset_index(drop=True)\n    \n    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n    valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], pin_memory=True, shuffle=True, drop_last=True, num_workers=4)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], pin_memory=True, shuffle=False, num_workers=4)\n    \n    return train_loader, valid_loader\n\ntrain_loader, valid_loader = prepared_loader(fold=0)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:55:30.102572Z","iopub.execute_input":"2021-11-09T07:55:30.102830Z","iopub.status.idle":"2021-11-09T07:55:30.117863Z","shell.execute_reply.started":"2021-11-09T07:55:30.102802Z","shell.execute_reply":"2021-11-09T07:55:30.117092Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def prepare_loader(fold):\n    df_train = df[df['kfold'] != fold].reset_index(drop=True)\n    df_valid = df[df['kfold'] == fold].reset_index(drop=True)\n    \n    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n    valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], pin_memory=True, shuffle=True, drop_last=True, num_workers=4)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], pin_memory=True, shuffle=False, num_workers=4)\n    \n    return train_loader, valid_loader\n\ntrain_loader, valid_loader = prepare_loader(fold=0)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T08:26:07.375732Z","iopub.execute_input":"2021-11-02T08:26:07.376427Z","iopub.status.idle":"2021-11-02T08:26:07.396999Z","shell.execute_reply.started":"2021-11-02T08:26:07.376386Z","shell.execute_reply":"2021-11-02T08:26:07.396171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='Pawpularity',\n                 config=CONFIG,\n                 job_type='Train',\n                 anonymous='must')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:55:33.184115Z","iopub.execute_input":"2021-11-09T07:55:33.184707Z","iopub.status.idle":"2021-11-09T07:55:40.598533Z","shell.execute_reply.started":"2021-11-09T07:55:33.184671Z","shell.execute_reply":"2021-11-09T07:55:40.597740Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model, history = run_training(model, optimizer, scheduler, device=CONFIG['device'], num_epochs=CONFIG['epochs'])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:56:14.702683Z","iopub.execute_input":"2021-11-09T07:56:14.702960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_file_path(id):\n    return f'{TEST_DIR}/{id}.jpg'\n\ntest_df = pd.read_csv(f'{ROOT_DIR}/test.csv')\ntest_df['file_path'] = test_df['Id'].apply(get_test_file_path)\n\nfeature_cols = [cols for cols test_df.columns if cols not in ['Id', 'file_path', 'Pawpularity']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularityTestDataset(Dataset):\n    def __init__(self, root_dir, df, transforms=None):\n        self.root_dir = root_dir\n        self.df = df\n        self.file_name = df['file_path'].values\n        self.meta = df[feature_cols].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_name[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        meta = self.meta[index, :]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return img, meta","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}