{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "optiver-realized-volatility-prediction_필사.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/O-Kpy/Kaggle/blob/main/optiver_realized_volatility_prediction_%ED%95%84%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T09:29:29.828778Z",
          "iopub.execute_input": "2021-09-17T09:29:29.829251Z",
          "iopub.status.idle": "2021-09-17T09:29:30.889645Z",
          "shell.execute_reply.started": "2021-09-17T09:29:29.829099Z",
          "shell.execute_reply": "2021-09-17T09:29:30.888437Z"
        },
        "trusted": true,
        "id": "nJbrK-SamqaH"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "import numpy.matlib\n",
        "\n",
        "path_submissions = '/'\n",
        "target_name='target'\n",
        "scores_folds={}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T09:29:32.147719Z",
          "iopub.execute_input": "2021-09-17T09:29:32.147992Z",
          "iopub.status.idle": "2021-09-17T09:29:32.217655Z",
          "shell.execute_reply.started": "2021-09-17T09:29:32.147965Z",
          "shell.execute_reply": "2021-09-17T09:29:32.216550Z"
        },
        "trusted": true,
        "id": "9h9FmtZlmqaJ"
      },
      "source": [
        "# data directory\n",
        "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
        "\n",
        "# Function to calculate first WAP\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate second WAP\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap3(df):\n",
        "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap4(df):\n",
        "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate the log of the return\n",
        "# Remember that logb(x / y) = logb(x) - logb(y)\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "# Calculate the realized volatility\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "# Function to count unique elements of a series\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))\n",
        "\n",
        "# Function to read our base train and test set\n",
        "def read_train_test():\n",
        "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
        "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    return train, test\n",
        "\n",
        "# Function to preprocess book data (for each stock id)\n",
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # Calculate Wap\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    df['wap3'] = calc_wap3(df)\n",
        "    df['wap4'] = calc_wap4(df)\n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
        "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
        "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
        "    # Calculate wap balance\n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    # Calculate spread\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.std],\n",
        "        'wap2': [np.sum, np.std],\n",
        "        'wap3': [np.sum, np.std],\n",
        "        'wap4': [np.sum, np.std],\n",
        "        'log_return1': [realized_volatility],\n",
        "        'log_return2': [realized_volatility],\n",
        "        'log_return3': [realized_volatility],\n",
        "        'log_return4': [realized_volatility],\n",
        "        'wap_balance': [np.sum, np.max],\n",
        "        'price_spread':[np.sum, np.max],\n",
        "        'price_spread2':[np.sum, np.max],\n",
        "        'bid_spread':[np.sum, np.max],\n",
        "        'ask_spread':[np.sum, np.max],\n",
        "        'total_volume':[np.sum, np.max],\n",
        "        'volume_imbalance':[np.sum, np.max],\n",
        "        \"bid_ask_spread\":[np.sum,  np.max],\n",
        "    }\n",
        "    create_feature_dict_time = {\n",
        "        'log_return1': [realized_volatility],\n",
        "        'log_return2': [realized_volatility],\n",
        "        'log_return3': [realized_volatility],\n",
        "        'log_return4': [realized_volatility],\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
        "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
        "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
        "\n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
        "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
        "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
        "    \n",
        "    \n",
        "    # Create row_id so we can merge\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "    df['amount']=df['price']*df['size']\n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum, np.max, np.min],\n",
        "        'order_count':[np.sum,np.max],\n",
        "        'amount':[np.sum,np.max,np.min],\n",
        "    }\n",
        "    create_feature_dict_time = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum],\n",
        "        'order_count':[np.sum],\n",
        "    }\n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "\n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
        "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
        "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
        "    \n",
        "    def tendency(price, vol):    \n",
        "        df_diff = np.diff(price)\n",
        "        val = (df_diff/price[1:])*100\n",
        "        power = np.sum(val*vol[1:])\n",
        "        return(power)\n",
        "    \n",
        "    lis = []\n",
        "    for n_time_id in df['time_id'].unique():\n",
        "        df_id = df[df['time_id'] == n_time_id]        \n",
        "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
        "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
        "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
        "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
        "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
        "        # new\n",
        "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
        "        energy = np.mean(df_id['price'].values**2)\n",
        "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
        "        \n",
        "        # vol vars\n",
        "        \n",
        "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
        "        energy_v = np.sum(df_id['size'].values**2)\n",
        "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
        "        \n",
        "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
        "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
        "    \n",
        "    df_lr = pd.DataFrame(lis)\n",
        "        \n",
        "   \n",
        "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
        "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
        "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
        "    \n",
        "    \n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "# Function to get group stats for the stock_id and time_id\n",
        "def get_time_stock(df):\n",
        "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
        "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
        "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
        "\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
        "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "    \n",
        "    # Merge with original dataframe\n",
        "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
        "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
        "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
        "    return df\n",
        "    \n",
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df\n",
        "\n",
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T09:29:37.464393Z",
          "iopub.execute_input": "2021-09-17T09:29:37.464846Z",
          "iopub.status.idle": "2021-09-17T11:14:39.946117Z",
          "shell.execute_reply.started": "2021-09-17T09:29:37.464817Z",
          "shell.execute_reply": "2021-09-17T11:14:39.944996Z"
        },
        "trusted": true,
        "id": "Gr3-9Zy9mqaP",
        "outputId": "26589beb-69df-4e4c-f552-4cd778b496b0"
      },
      "source": [
        "train, test = read_train_test()\n",
        "\n",
        "# train\n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "\n",
        "# preprocess them using parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on=['row_id'], how='left')\n",
        "\n",
        "# test\n",
        "test_stock_ids = test['stock_id'].unique(\n",
        ")\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on=['row_id'], how='left')\n",
        "\n",
        "train = get_time_stock(train)\n",
        "test = get_time_stock(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Our training set has 428932 rows\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 43.2min\n[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 104.9min finished\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T11:14:39.949389Z",
          "iopub.execute_input": "2021-09-17T11:14:39.950119Z",
          "iopub.status.idle": "2021-09-17T11:14:40.009531Z",
          "shell.execute_reply.started": "2021-09-17T11:14:39.950061Z",
          "shell.execute_reply": "2021-09-17T11:14:40.008612Z"
        },
        "trusted": true,
        "id": "9HTbZjWBmqaR"
      },
      "source": [
        "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
        "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
        "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
        "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
        "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
        "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
        "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
        "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
        "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
        "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
        "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
        "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
        "\n",
        "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
        "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
        "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
        "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
        "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
        "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
        "\n",
        "# delta tau\n",
        "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
        "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T11:14:40.011151Z",
          "iopub.execute_input": "2021-09-17T11:14:40.011500Z",
          "iopub.status.idle": "2021-09-17T11:14:42.245176Z",
          "shell.execute_reply.started": "2021-09-17T11:14:40.011434Z",
          "shell.execute_reply": "2021-09-17T11:14:42.243890Z"
        },
        "trusted": true,
        "id": "93laPAUWmqaR",
        "outputId": "d8ea848f-b129-409c-b046-5c54ed245fb0"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# making agg features\n",
        "\n",
        "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
        "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "corr = train_p.corr()\n",
        "\n",
        "ids = corr.index\n",
        "\n",
        "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
        "print(kmeans.labels_)\n",
        "\n",
        "l = []\n",
        "for n in range(7):\n",
        "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "    \n",
        "\n",
        "mat = []\n",
        "matTest = []\n",
        "\n",
        "n = 0\n",
        "for ind in l:\n",
        "    print(ind)\n",
        "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    mat.append ( newDf )\n",
        "    \n",
        "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    matTest.append ( newDf )\n",
        "    \n",
        "    n+=1\n",
        "    \n",
        "mat1 = pd.concat(mat).reset_index()\n",
        "mat1.drop(columns=['target'],inplace=True)\n",
        "\n",
        "mat2 = pd.concat(matTest).reset_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n 0]\n[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n[3, 6, 9, 18, 61, 63, 86, 97]\n[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n[81]\n[8, 80]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T11:14:42.248638Z",
          "iopub.execute_input": "2021-09-17T11:14:42.248991Z",
          "iopub.status.idle": "2021-09-17T11:14:42.458335Z",
          "shell.execute_reply.started": "2021-09-17T11:14:42.248950Z",
          "shell.execute_reply": "2021-09-17T11:14:42.457190Z"
        },
        "trusted": true,
        "id": "WWEdtGNXmqaS",
        "outputId": "2af5af07-b2b7-4a07-cb84-94efae2683eb"
      },
      "source": [
        "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
        "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
        "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
        "mat1.reset_index(inplace=True)\n",
        "\n",
        "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
        "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
        "mat2.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n  This is separate from the ipykernel package so we can avoid doing imports until\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n  import sys\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T11:14:42.460431Z",
          "iopub.execute_input": "2021-09-17T11:14:42.460802Z",
          "iopub.status.idle": "2021-09-17T11:14:51.137599Z",
          "shell.execute_reply.started": "2021-09-17T11:14:42.460735Z",
          "shell.execute_reply": "2021-09-17T11:14:51.136586Z"
        },
        "trusted": true,
        "id": "4uFt4pJEmqaS",
        "outputId": "013c695b-213e-43f1-c8c4-9729c5465584"
      },
      "source": [
        "nnn = ['time_id',\n",
        "     'log_return1_realized_volatility_0c1',\n",
        "     'log_return1_realized_volatility_1c1',     \n",
        "     'log_return1_realized_volatility_3c1',\n",
        "     'log_return1_realized_volatility_4c1',     \n",
        "     'log_return1_realized_volatility_6c1',\n",
        "     'total_volume_sum_0c1',\n",
        "     'total_volume_sum_1c1', \n",
        "     'total_volume_sum_3c1',\n",
        "     'total_volume_sum_4c1', \n",
        "     'total_volume_sum_6c1',\n",
        "     'trade_size_sum_0c1',\n",
        "     'trade_size_sum_1c1', \n",
        "     'trade_size_sum_3c1',\n",
        "     'trade_size_sum_4c1', \n",
        "     'trade_size_sum_6c1',\n",
        "     'trade_order_count_sum_0c1',\n",
        "     'trade_order_count_sum_1c1',\n",
        "     'trade_order_count_sum_3c1',\n",
        "     'trade_order_count_sum_4c1',\n",
        "     'trade_order_count_sum_6c1',      \n",
        "     'price_spread_sum_0c1',\n",
        "     'price_spread_sum_1c1',\n",
        "     'price_spread_sum_3c1',\n",
        "     'price_spread_sum_4c1',\n",
        "     'price_spread_sum_6c1',   \n",
        "     'bid_spread_sum_0c1',\n",
        "     'bid_spread_sum_1c1',\n",
        "     'bid_spread_sum_3c1',\n",
        "     'bid_spread_sum_4c1',\n",
        "     'bid_spread_sum_6c1',       \n",
        "     'ask_spread_sum_0c1',\n",
        "     'ask_spread_sum_1c1',\n",
        "     'ask_spread_sum_3c1',\n",
        "     'ask_spread_sum_4c1',\n",
        "     'ask_spread_sum_6c1',   \n",
        "     'volume_imbalance_sum_0c1',\n",
        "     'volume_imbalance_sum_1c1',\n",
        "     'volume_imbalance_sum_3c1',\n",
        "     'volume_imbalance_sum_4c1',\n",
        "     'volume_imbalance_sum_6c1',       \n",
        "     'bid_ask_spread_sum_0c1',\n",
        "     'bid_ask_spread_sum_1c1',\n",
        "     'bid_ask_spread_sum_3c1',\n",
        "     'bid_ask_spread_sum_4c1',\n",
        "     'bid_ask_spread_sum_6c1',\n",
        "     'size_tau2_0c1',\n",
        "     'size_tau2_1c1',\n",
        "     'size_tau2_3c1',\n",
        "     'size_tau2_4c1',\n",
        "     'size_tau2_6c1'] \n",
        "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
        "test = pd.merge(test,mat2[nnn],how='left',on='time_id')\n",
        "\n",
        "import gc\n",
        "del mat1,mat2\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T09:10:30.082279Z",
          "iopub.execute_input": "2021-09-17T09:10:30.083452Z",
          "iopub.status.idle": "2021-09-17T09:10:30.242433Z",
          "shell.execute_reply.started": "2021-09-17T09:10:30.083372Z",
          "shell.execute_reply": "2021-09-17T09:10:30.241471Z"
        },
        "trusted": true,
        "id": "ytilP19smqaT",
        "outputId": "193fe729-11a5-42cd-c313-709e5094d2a9"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "        stock_id  time_id    target     row_id    wap1_sum  wap1_std  \\\n0              0        5  0.004136        0-5  303.125061  0.000693   \n1              0       11  0.001445       0-11  200.047768  0.000262   \n2              0       16  0.002168       0-16  187.913849  0.000864   \n3              0       31  0.002195       0-31  119.859781  0.000757   \n4              0       62  0.001747       0-62  175.932865  0.000258   \n...          ...      ...       ...        ...         ...       ...   \n428927       126    32751  0.003461  126-32751  309.870453  0.000486   \n428928       126    32753  0.003113  126-32753  223.552139  0.001264   \n428929       126    32758  0.004070  126-32758  256.277039  0.000466   \n428930       126    32763  0.003357  126-32763  399.721741  0.000456   \n428931       126    32767  0.002090  126-32767  217.058914  0.000384   \n\n          wap2_sum  wap2_std    wap3_sum  wap3_std  ...  \\\n0       303.105539  0.000781  303.134863  0.000637  ...   \n1       200.041171  0.000272  200.035611  0.000298  ...   \n2       187.939824  0.000862  187.923063  0.000670  ...   \n3       119.835941  0.000656  119.870163  0.000606  ...   \n4       175.934256  0.000317  175.928283  0.000215  ...   \n...            ...       ...         ...       ...  ...   \n428927  309.871368  0.000613  309.778625  0.000596  ...   \n428928  223.580322  0.001303  223.505493  0.001340  ...   \n428929  256.255066  0.000599  256.146057  0.000520  ...   \n428930  399.714325  0.000507  399.775391  0.000424  ...   \n428931  217.079727  0.000465  217.069031  0.000341  ...   \n\n        bid_ask_spread_sum_0c1  bid_ask_spread_sum_1c1  \\\n0                     0.118397                0.113150   \n1                     0.072559                0.071506   \n2                     0.079010                0.091842   \n3                     0.072684                0.075466   \n4                     0.076716                0.073103   \n...                        ...                     ...   \n428927                0.088530                0.091100   \n428928                0.077133                0.072731   \n428929                0.070713                0.086828   \n428930                0.110701                0.127982   \n428931                0.075224                0.088471   \n\n        bid_ask_spread_sum_3c1  bid_ask_spread_sum_4c1  \\\n0                     0.174687                0.155552   \n1                     0.122967                0.110758   \n2                     0.158230                0.142469   \n3                     0.133869                0.109418   \n4                     0.109770                0.110781   \n...                        ...                     ...   \n428927                0.148065                0.140075   \n428928                0.128888                0.121852   \n428929                0.148556                0.138048   \n428930                0.204786                0.185053   \n428931                0.149643                0.141470   \n\n        bid_ask_spread_sum_6c1  size_tau2_0c1  size_tau2_1c1  size_tau2_3c1  \\\n0                     0.175661       0.058550       0.057267       0.078471   \n1                     0.099451       0.081235       0.078955       0.122289   \n2                     0.088431       0.078550       0.087378       0.116278   \n3                     0.139281       0.100382       0.089673       0.105948   \n4                     0.177286       0.087285       0.089068       0.112663   \n...                        ...            ...            ...            ...   \n428927                0.163514       0.064380       0.064007       0.089665   \n428928                0.100756       0.073644       0.065909       0.085298   \n428929                0.080120       0.098654       0.092117       0.124247   \n428930                0.194994       0.051648       0.054263       0.067649   \n428931                0.128776       0.077370       0.078584       0.108765   \n\n        size_tau2_4c1  size_tau2_6c1  \n0            0.054691       0.050700  \n1            0.078616       0.045740  \n2            0.074977       0.080722  \n3            0.094684       0.055447  \n4            0.086381       0.046358  \n...               ...            ...  \n428927       0.061596       0.041157  \n428928       0.062453       0.062479  \n428929       0.090574       0.120507  \n428930       0.051358       0.040747  \n428931       0.079125       0.053870  \n\n[428932 rows x 248 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>time_id</th>\n      <th>target</th>\n      <th>row_id</th>\n      <th>wap1_sum</th>\n      <th>wap1_std</th>\n      <th>wap2_sum</th>\n      <th>wap2_std</th>\n      <th>wap3_sum</th>\n      <th>wap3_std</th>\n      <th>...</th>\n      <th>bid_ask_spread_sum_0c1</th>\n      <th>bid_ask_spread_sum_1c1</th>\n      <th>bid_ask_spread_sum_3c1</th>\n      <th>bid_ask_spread_sum_4c1</th>\n      <th>bid_ask_spread_sum_6c1</th>\n      <th>size_tau2_0c1</th>\n      <th>size_tau2_1c1</th>\n      <th>size_tau2_3c1</th>\n      <th>size_tau2_4c1</th>\n      <th>size_tau2_6c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5</td>\n      <td>0.004136</td>\n      <td>0-5</td>\n      <td>303.125061</td>\n      <td>0.000693</td>\n      <td>303.105539</td>\n      <td>0.000781</td>\n      <td>303.134863</td>\n      <td>0.000637</td>\n      <td>...</td>\n      <td>0.118397</td>\n      <td>0.113150</td>\n      <td>0.174687</td>\n      <td>0.155552</td>\n      <td>0.175661</td>\n      <td>0.058550</td>\n      <td>0.057267</td>\n      <td>0.078471</td>\n      <td>0.054691</td>\n      <td>0.050700</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>11</td>\n      <td>0.001445</td>\n      <td>0-11</td>\n      <td>200.047768</td>\n      <td>0.000262</td>\n      <td>200.041171</td>\n      <td>0.000272</td>\n      <td>200.035611</td>\n      <td>0.000298</td>\n      <td>...</td>\n      <td>0.072559</td>\n      <td>0.071506</td>\n      <td>0.122967</td>\n      <td>0.110758</td>\n      <td>0.099451</td>\n      <td>0.081235</td>\n      <td>0.078955</td>\n      <td>0.122289</td>\n      <td>0.078616</td>\n      <td>0.045740</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>16</td>\n      <td>0.002168</td>\n      <td>0-16</td>\n      <td>187.913849</td>\n      <td>0.000864</td>\n      <td>187.939824</td>\n      <td>0.000862</td>\n      <td>187.923063</td>\n      <td>0.000670</td>\n      <td>...</td>\n      <td>0.079010</td>\n      <td>0.091842</td>\n      <td>0.158230</td>\n      <td>0.142469</td>\n      <td>0.088431</td>\n      <td>0.078550</td>\n      <td>0.087378</td>\n      <td>0.116278</td>\n      <td>0.074977</td>\n      <td>0.080722</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>31</td>\n      <td>0.002195</td>\n      <td>0-31</td>\n      <td>119.859781</td>\n      <td>0.000757</td>\n      <td>119.835941</td>\n      <td>0.000656</td>\n      <td>119.870163</td>\n      <td>0.000606</td>\n      <td>...</td>\n      <td>0.072684</td>\n      <td>0.075466</td>\n      <td>0.133869</td>\n      <td>0.109418</td>\n      <td>0.139281</td>\n      <td>0.100382</td>\n      <td>0.089673</td>\n      <td>0.105948</td>\n      <td>0.094684</td>\n      <td>0.055447</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>62</td>\n      <td>0.001747</td>\n      <td>0-62</td>\n      <td>175.932865</td>\n      <td>0.000258</td>\n      <td>175.934256</td>\n      <td>0.000317</td>\n      <td>175.928283</td>\n      <td>0.000215</td>\n      <td>...</td>\n      <td>0.076716</td>\n      <td>0.073103</td>\n      <td>0.109770</td>\n      <td>0.110781</td>\n      <td>0.177286</td>\n      <td>0.087285</td>\n      <td>0.089068</td>\n      <td>0.112663</td>\n      <td>0.086381</td>\n      <td>0.046358</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>428927</th>\n      <td>126</td>\n      <td>32751</td>\n      <td>0.003461</td>\n      <td>126-32751</td>\n      <td>309.870453</td>\n      <td>0.000486</td>\n      <td>309.871368</td>\n      <td>0.000613</td>\n      <td>309.778625</td>\n      <td>0.000596</td>\n      <td>...</td>\n      <td>0.088530</td>\n      <td>0.091100</td>\n      <td>0.148065</td>\n      <td>0.140075</td>\n      <td>0.163514</td>\n      <td>0.064380</td>\n      <td>0.064007</td>\n      <td>0.089665</td>\n      <td>0.061596</td>\n      <td>0.041157</td>\n    </tr>\n    <tr>\n      <th>428928</th>\n      <td>126</td>\n      <td>32753</td>\n      <td>0.003113</td>\n      <td>126-32753</td>\n      <td>223.552139</td>\n      <td>0.001264</td>\n      <td>223.580322</td>\n      <td>0.001303</td>\n      <td>223.505493</td>\n      <td>0.001340</td>\n      <td>...</td>\n      <td>0.077133</td>\n      <td>0.072731</td>\n      <td>0.128888</td>\n      <td>0.121852</td>\n      <td>0.100756</td>\n      <td>0.073644</td>\n      <td>0.065909</td>\n      <td>0.085298</td>\n      <td>0.062453</td>\n      <td>0.062479</td>\n    </tr>\n    <tr>\n      <th>428929</th>\n      <td>126</td>\n      <td>32758</td>\n      <td>0.004070</td>\n      <td>126-32758</td>\n      <td>256.277039</td>\n      <td>0.000466</td>\n      <td>256.255066</td>\n      <td>0.000599</td>\n      <td>256.146057</td>\n      <td>0.000520</td>\n      <td>...</td>\n      <td>0.070713</td>\n      <td>0.086828</td>\n      <td>0.148556</td>\n      <td>0.138048</td>\n      <td>0.080120</td>\n      <td>0.098654</td>\n      <td>0.092117</td>\n      <td>0.124247</td>\n      <td>0.090574</td>\n      <td>0.120507</td>\n    </tr>\n    <tr>\n      <th>428930</th>\n      <td>126</td>\n      <td>32763</td>\n      <td>0.003357</td>\n      <td>126-32763</td>\n      <td>399.721741</td>\n      <td>0.000456</td>\n      <td>399.714325</td>\n      <td>0.000507</td>\n      <td>399.775391</td>\n      <td>0.000424</td>\n      <td>...</td>\n      <td>0.110701</td>\n      <td>0.127982</td>\n      <td>0.204786</td>\n      <td>0.185053</td>\n      <td>0.194994</td>\n      <td>0.051648</td>\n      <td>0.054263</td>\n      <td>0.067649</td>\n      <td>0.051358</td>\n      <td>0.040747</td>\n    </tr>\n    <tr>\n      <th>428931</th>\n      <td>126</td>\n      <td>32767</td>\n      <td>0.002090</td>\n      <td>126-32767</td>\n      <td>217.058914</td>\n      <td>0.000384</td>\n      <td>217.079727</td>\n      <td>0.000465</td>\n      <td>217.069031</td>\n      <td>0.000341</td>\n      <td>...</td>\n      <td>0.075224</td>\n      <td>0.088471</td>\n      <td>0.149643</td>\n      <td>0.141470</td>\n      <td>0.128776</td>\n      <td>0.077370</td>\n      <td>0.078584</td>\n      <td>0.108765</td>\n      <td>0.079125</td>\n      <td>0.053870</td>\n    </tr>\n  </tbody>\n</table>\n<p>428932 rows × 248 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T11:14:51.139734Z",
          "iopub.execute_input": "2021-09-17T11:14:51.140069Z",
          "iopub.status.idle": "2021-09-17T11:14:52.486469Z",
          "shell.execute_reply.started": "2021-09-17T11:14:51.140027Z",
          "shell.execute_reply": "2021-09-17T11:14:52.485235Z"
        },
        "trusted": true,
        "id": "o1PKJ0pumqaU"
      },
      "source": [
        "import optuna\n",
        "import catboost\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train = train.drop(columns=[\"time_id\", \"target\", \"row_id\"])\n",
        "y_train = train['target']\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
        "\n",
        "cat_features = x_train.select_dtypes(include='object')\n",
        "\n",
        "def objective(trial,data=x_train,target=y_train):\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2,random_state=42)\n",
        "    params = {'iterations':trial.suggest_int(\"iterations\", 2000, 40000, step=1000),\n",
        "              'eval_metric':'RMSE',\n",
        "              'leaf_estimation_method':'Newton',\n",
        "              'bootstrap_type': 'Bernoulli',\n",
        "              'learning_rate' : trial.suggest_uniform('learning_rate',0.01,0.5),\n",
        "              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n",
        "              'subsample': trial.suggest_uniform('subsample',0,1),\n",
        "              'random_strength': trial.suggest_uniform('random_strength',10,50),\n",
        "              'depth': trial.suggest_int('depth',1,13),\n",
        "              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,50),\n",
        "              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n",
        "              'grow_policy' : 'Depthwise',\n",
        "              'task_type':\"GPU\",\n",
        "              # 'cat_features':cat_features\n",
        "               }\n",
        "    model = catboost.CatBoostRegressor(**params)  \n",
        "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n",
        "        \n",
        "    y_preds = model.predict(X_test)\n",
        "\n",
        "\n",
        "    RMSPE = rmspe(y_test, y_preds)\n",
        "    \n",
        "    return RMSPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T11:14:52.487901Z",
          "iopub.execute_input": "2021-09-17T11:14:52.490337Z"
        },
        "trusted": true,
        "id": "bXOefHPmmqaU",
        "outputId": "8e88fbc2-6e5d-4fd4-d95f-e81078b47289"
      },
      "source": [
        "OPTUNA_OPTIMIZATION = True\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "print('Number of finished trials:', len(study.trials))\n",
        "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[32m[I 2021-09-17 11:14:52,492]\u001b[0m A new study created in memory with name: no-name-55ec894b-5ff8-495d-8118-0b5e1cf970c5\u001b[0m\n\u001b[32m[I 2021-09-17 11:17:00,221]\u001b[0m Trial 0 finished with value: 0.23774228932487687 and parameters: {'iterations': 21000, 'learning_rate': 0.32065670345980035, 'reg_lambda': 44.161576958957085, 'subsample': 0.8691541361503792, 'random_strength': 49.87665579041711, 'depth': 2, 'min_data_in_leaf': 10, 'leaf_estimation_iterations': 11}. Best is trial 0 with value: 0.23774228932487687.\u001b[0m\n\u001b[32m[I 2021-09-17 11:17:38,131]\u001b[0m Trial 1 finished with value: 0.23063860897410707 and parameters: {'iterations': 29000, 'learning_rate': 0.2128014546930395, 'reg_lambda': 21.65820608423128, 'subsample': 0.8281462609629872, 'random_strength': 19.139303097321992, 'depth': 3, 'min_data_in_leaf': 18, 'leaf_estimation_iterations': 8}. Best is trial 1 with value: 0.23063860897410707.\u001b[0m\n\u001b[32m[I 2021-09-17 11:19:35,935]\u001b[0m Trial 2 finished with value: 0.26590541779872734 and parameters: {'iterations': 8000, 'learning_rate': 0.42475172677587963, 'reg_lambda': 85.45025499691107, 'subsample': 0.2753930826749381, 'random_strength': 12.656766033522477, 'depth': 1, 'min_data_in_leaf': 29, 'leaf_estimation_iterations': 13}. Best is trial 1 with value: 0.23063860897410707.\u001b[0m\n\u001b[32m[I 2021-09-17 11:21:02,198]\u001b[0m Trial 3 finished with value: 0.22738160346453692 and parameters: {'iterations': 29000, 'learning_rate': 0.09401839035772784, 'reg_lambda': 40.50125400233862, 'subsample': 0.28912222107837204, 'random_strength': 13.965113923112963, 'depth': 6, 'min_data_in_leaf': 9, 'leaf_estimation_iterations': 3}. Best is trial 3 with value: 0.22738160346453692.\u001b[0m\n\u001b[32m[I 2021-09-17 11:21:31,196]\u001b[0m Trial 4 finished with value: 0.24617509503437074 and parameters: {'iterations': 17000, 'learning_rate': 0.4912823850447295, 'reg_lambda': 54.762807781212, 'subsample': 0.3343008194211843, 'random_strength': 20.82963018711967, 'depth': 3, 'min_data_in_leaf': 19, 'leaf_estimation_iterations': 12}. Best is trial 3 with value: 0.22738160346453692.\u001b[0m\n\u001b[32m[I 2021-09-17 11:21:45,897]\u001b[0m Trial 5 finished with value: 0.2495023642389249 and parameters: {'iterations': 35000, 'learning_rate': 0.42874581836150405, 'reg_lambda': 17.383223249046246, 'subsample': 0.16042535137267278, 'random_strength': 24.14654716776679, 'depth': 7, 'min_data_in_leaf': 50, 'leaf_estimation_iterations': 14}. Best is trial 3 with value: 0.22738160346453692.\u001b[0m\n\u001b[32m[I 2021-09-17 11:22:06,016]\u001b[0m Trial 6 finished with value: 0.23978316628605015 and parameters: {'iterations': 16000, 'learning_rate': 0.38138577177697297, 'reg_lambda': 49.47982598417109, 'subsample': 0.21044173325733018, 'random_strength': 49.997027308504805, 'depth': 12, 'min_data_in_leaf': 33, 'leaf_estimation_iterations': 6}. Best is trial 3 with value: 0.22738160346453692.\u001b[0m\n\u001b[32m[I 2021-09-17 11:24:32,969]\u001b[0m Trial 7 finished with value: 0.25084655592025745 and parameters: {'iterations': 9000, 'learning_rate': 0.09690426750865735, 'reg_lambda': 77.77847233727871, 'subsample': 0.024270999679076755, 'random_strength': 43.09567719494492, 'depth': 6, 'min_data_in_leaf': 20, 'leaf_estimation_iterations': 13}. Best is trial 3 with value: 0.22738160346453692.\u001b[0m\n\u001b[32m[I 2021-09-17 11:25:24,567]\u001b[0m Trial 8 finished with value: 0.2292908621221371 and parameters: {'iterations': 6000, 'learning_rate': 0.19280772251997694, 'reg_lambda': 43.89494629601224, 'subsample': 0.9601718436994463, 'random_strength': 27.55294372879623, 'depth': 3, 'min_data_in_leaf': 6, 'leaf_estimation_iterations': 14}. Best is trial 3 with value: 0.22738160346453692.\u001b[0m\n\u001b[32m[I 2021-09-17 11:26:10,837]\u001b[0m Trial 9 finished with value: 0.2703449372953118 and parameters: {'iterations': 7000, 'learning_rate': 0.3638204149726257, 'reg_lambda': 49.35173190908851, 'subsample': 0.11553107254590023, 'random_strength': 16.633624662395384, 'depth': 1, 'min_data_in_leaf': 40, 'leaf_estimation_iterations': 3}. Best is trial 3 with value: 0.22738160346453692.\u001b[0m\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}